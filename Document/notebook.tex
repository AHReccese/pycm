
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Document}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \hypertarget{pycm-document}{%
\section{PyCM Document}\label{pycm-document}}

    \hypertarget{version-1.4}{%
\subsection{\#\#\# Version : 1.4}\label{version-1.4}}

    \hypertarget{table-of-contents}{%
\subsection{Table of contents}\label{table-of-contents}}

    Overview

Installation

\begin{verbatim}
<ol>
        <li><a href="#Source-Code">Source Code</a></li>
        <li><a href="#PyPI">PyPI</a></li>
        <li><a href="#Easy-Install">Easy Install</a></li>
    </ol>
&nbsp;
<li><a href="#Usage">Usage</a></li>
    <ol>
        <li><a href="#From-Vector">From Vector</a></li>
        <li><a href="#Direct-CM">Direct CM</a></li>
        <li><a href="#Activation-Threshold">Activation Threshold</a></li>
        <li><a href="#Load-From-File">Load From File</a></li>
        <li><a href="#Sample-Weights">Sample Weights</a></li>
        <li><a href="#Transpose">Transpose</a></li>
        <li><a href="#Relabel">Relabel</a></li>
        <li><a href="#Online-Help">Online Help</a></li>
        <li><a href="#Acceptable-Data-Types">Acceptable Data Types</a></li>
    </ol>
&nbsp;
<li><a href="#Basic-Parameters">Basic Parameters</a></li>
<ol>
    <li><a href="#TP-(True-positive-/-hit)">True Positive</a></li>
    <li><a href="#TN-(True-negative/correct-rejection)">True Negative</a></li>
    <li><a href="#FP-(False-positive/false-alarm/Type-I-error)">False Positive</a></li>
    <li><a href="#FN-(False-negative/miss/Type-II-error)">False Negative</a></li>
    <li><a href="#P-(Condition-positive)">Condition Positive</a></li>
    <li><a href="#N-(Condition-negative)">Condition Negative</a></li>
    <li><a href="#TOP-(Test-outcome-positive)">Test Outcome Positive</a></li>
    <li><a href="#TON-(Test-outcome-negative)">Test Outcome Negative</a></li>
    <li><a href="#POP-(Population)">Population</a></li>
</ol>
&nbsp;
<li><a href="#Class-Statistics">Class Statistics</a></li>
<ol>
    <li><a href="#TPR--(sensitivity,-recall,-hit-rate,-or-true-positive-rate)">True Positive Rate</a></li>
    <li><a href="#TNR-(specificity-or-true-negative-rate)">True Negative Rate</a></li>
    <li><a href="#PPV-(precision-or-positive-predictive-value)">Positive Predictive Value</a></li>
    <li><a href="#NPV-(negative-predictive-value)">Negative Predictive Value</a></li>
    <li><a href="#FNR-(miss-rate-or-false-negative-rate)">False Negative Rate</a></li>
    <li><a href="#FPR-(fall-out-or-false-positive-rate)">False Positive Rate</a></li>
    <li><a href="#FDR-(false-discovery-rate)">False Discovery Rate</a></li>
    <li><a href="#FOR-(false-omission-rate)">False Omission Rate</a></li>
    <li><a href="#ACC-(accuracy)">Accuracy</a></li>
    <li><a href="#ERR(Error-rate)">Error Rate</a></li>
    <li><a href="#FBeta-Score">FBeta Score</a></li>
    <li><a href="#MCC-(Matthews-correlation-coefficient)">Matthews Correlation Coefficient</a></li>
    <li><a href="#BM-(Informedness-or-Bookmaker-Informedness)">Informedness</a></li>
    <li><a href="#MK-(Markedness)">Markedness</a></li>
    <li><a href="#PLR-(Positive-likelihood-ratio)">Positive Likelihood Ratio</a></li>
    <li><a href="#NLR-(Negative-likelihood-ratio)">Negative Likelihood Ratio</a></li>
    <li><a href="#DOR-(Diagnostic-odds-ratio)">Diagnostic Odds Ratio</a></li>
    <li><a href="#PRE-(Prevalence)">Prevalence</a></li>
    <li><a href="#G-(G-measure-geometric-mean-of-precision-and-sensitivity)">G-Measure</a></li>
    <li><a href="#RACC(Random-accuracy)">Random Accuracy</a></li>
    <li><a href="#RACCU(Random-accuracy-unbiased)">Random Accuracy Unbiased</a></li>
    <li><a href="#J-(Jaccard-index)">Jaccard Index</a></li>
    <li><a href="#IS-(Information-Score)">Information Score</a></li>
    <li><a href="#CEN-(Confusion-Entropy)">Confusion Entropy</a></li>
    <li><a href="#MCEN-(Modified-Confusion-Entropy)">Modified Confusion Entropy</a></li>
    <li><a href="#AUC-(Area-Under-The-ROC-Curve)">Area Under The ROC Curve</a></li>
    <li><a href="#dInd-(Distance-Index)">Distance Index</a></li>
    <li><a href="#sInd-(Similarity-Index)">Similarity Index</a></li>
    <li><a href="#DP-(Discriminant-Power)">Discriminant Power</a></li>
    <li><a href="#Y-(Youden-Index)">Youden Index</a></li>
    <li><a href="#PLRI-(Positive-likelihood-ratio-interpretation)">Positive Likelihood Ratio Interpretation</a></li>
    <li><a href="#DPI-(Discriminant-power-interpretation)">Discriminant Power Interpretation</a></li>
</ol>
&nbsp;
<li><a href="#Overall-Statistics">Overall Statistics</a></li>
<ol>
    <li><a href="#Kappa-(Nominal)">Kappa</a></li>
    <li><a href="#Kappa-Unbiased">Kappa Unbiased</a></li>
    <li><a href="#Kappa-No-Prevalence">Kappa No Prevalence</a></li>
    <li><a href="#Kappa-95%-CI">Kappa 95% CI</a></li>
    <li><a href="#Kappa-95%-CI">Kappa Standard Error</a></li>
    <li><a href="#Chi-Squared">Chi Squared</a></li>
    <li><a href="#Chi-Squared-DF">Chi Squared DF</a></li>
    <li><a href="#Phi-Squared">Phi Squared</a></li>
    <li><a href="#Cramer's-V">Cramer's V</a></li>
    <li><a href="#95%-CI">95% CI</a></li>
    <li><a href="#95%-CI">Standard Error</a></li>
    <li><a href="#Bennett-et-al.'s-S-score-(Nominal)">Bennett et al.'s S Score</a></li>
    <li><a href="#Scott's-pi-(Nominal)">Scott's Pi</a></li>
    <li><a href="#Gwet's-AC1">Gwet's AC1</a></li>
    <li><a href="#Reference-Entropy">Reference Entropy</a></li>
    <li><a href="#Response-Entropy">Response Entropy</a></li>
    <li><a href="#Cross-Entropy">Cross Entropy</a></li>
    <li><a href="#Joint-Entropy">Joint Entropy</a></li>
    <li><a href="#Conditional-Entropy">Conditional Entropy</a></li>
    <li><a href="#Kullback-Liebler-(KL)-divergence">Kullback-Liebler Divergence</a></li>
    <li><a href="#Mutual-Information">Mutual Information</a></li>
    <li><a href="#Goodman-and-Kruskal's-lambda-A">Goodman-Kruskal's Lambda A</a></li>
    <li><a href="#Goodman-and-Kruskal's-lambda-B">Goodman-Kruskal's Lambda B</a></li>
    <li><a href="#SOA1-(Strength-of-Agreement,-Landis-and-Koch-benchmark)">Landis-Koch Benchmark</a></li>
    <li><a href="#SOA2-(Strength-of-Agreement,-:-Fleiss’-benchmark)">Fleiss’s Benchmark</a></li>
    <li><a href="#SOA3-(Strength-of-Agreement,-Altman’s-benchmark)">Altman’s Benchmark</a></li>
    <li><a href="#SOA4-(Strength-of-Agreement,-Cicchetti’s-benchmark)">Cicchetti’s Benchmark</a></li>
    <li><a href="#Overall_ACC">Overall Accuracy</a></li>
    <li><a href="#Overall_RACC">Overall Random Accuracy</a></li>
    <li><a href="#Overall_RACCU">Overall Random Accuracy Unbiased</a></li>
    <li><a href="#PPV_Micro">Positive Predictive Value Micro</a></li>
    <li><a href="#TPR_Micro">True Positive Rate Micro</a></li>
    <li><a href="#PPV_Macro">Positive Predictive Value Macro</a></li>
    <li><a href="#TPR_Macro">True Positive Rate Macro</a></li>
    <li><a href="#Overall_J">Overall Jaccard Index</a></li>
    <li><a href="#Hamming-Loss">Hamming Loss</a></li>
    <li><a href="#Zero-one-Loss">Zero-one Loss</a></li>
    <li><a href="#NIR-(No-Information-Rate)">No Information Rate</a></li>
    <li><a href="#P-Value">P Value</a></li>
    <li><a href="#Overall_CEN">Overall Confusion Entropy</a></li>
    <li><a href="#Overall_MCEN">Overall Modified Confusion Entropy</a></li>
    <li><a href="#Overall_MCC">Overall Matthews Correlation Coefficient</a></li>
    <li><a href="#RR-(Global-Performance-Index)">Global Performance Index</a></li>
    <li><a href="#CBA-(Class-Balance-Accuracy)">Class Balance Accuracy</a></li>
    <li><a href="#AUNU">AUNU</a></li>
    <li><a href="#AUNP">AUNP</a></li>
    <li><a href="#RCI-(Relative-Classifier-Information)">Relative Classifier Information</a></li>
</ol>
&nbsp;
<li><a href="#Print">Print</a></li>
<ol>
    <li><a href="#Full">Full</a></li>
    <li><a href="#Matrix">Matrix</a></li>
    <li><a href="#Normalized-Matrix">Normalized Matrix</a></li>
    <li><a href="#Stat">Stat</a></li>
</ol>
&nbsp;
<li><a href="#Save">Save</a></li>
<ol>
    <li><a href="#.pycm-file">pycm</a></li>
    <li><a href="#HTML">HTML</a></li>
    <li><a href="#CSV">CSV</a></li>
    <li><a href="#OBJ">Object</a></li>
</ol>
&nbsp;
<li><a href="#Input-Errors">Input Errors</a></li>
<li><a href="#Examples">Examples</a></li>
<li><a href="#References">References</a></li>
\end{verbatim}

    \hypertarget{overview}{%
\subsection{Overview}\label{overview}}

    PyCM is a multi-class confusion matrix library written in Python that
supports both input data vectors and direct matrix, and a proper tool
for post-classification model evaluation that supports most classes and
overall statistics parameters.\\
PyCM is the swiss-army knife of confusion matrices, targeted mainly at
data scientists that need a broad array of metrics for predictive models
and an accurate evaluation of large variety of classifiers.

    \begin{verbatim}
<img src="../Otherfiles/block_diagram.jpg">
\end{verbatim}

    \hypertarget{installation}{%
\subsection{Installation}\label{installation}}

    \hypertarget{source-code}{%
\subsubsection{Source Code}\label{source-code}}

\begin{itemize}
\tightlist
\item
  Download
  \href{https://github.com/sepandhaghighi/pycm/archive/v1.4.zip}{Version
  1.4} or
  \href{https://github.com/sepandhaghighi/pycm/archive/dev.zip}{Latest
  Source}
\item
  Run \texttt{pip\ install\ -r\ requirements.txt} or
  \texttt{pip3\ install\ -r\ requirements.txt} (Need root access)
\item
  Run \texttt{python3\ setup.py\ install} or
  \texttt{python\ setup.py\ install} (Need root access)
\end{itemize}

    \hypertarget{pypi}{%
\subsubsection{PyPI}\label{pypi}}

\begin{itemize}
\tightlist
\item
  Check \href{https://packaging.python.org/installing/}{Python Packaging
  User Guide}\\
\item
  Run \texttt{pip\ install\ pycm\ -\/-upgrade} or
  \texttt{pip3\ install\ pycm\ -\/-upgrade} (Need root access)
\end{itemize}

    \hypertarget{easy-install}{%
\subsubsection{Easy Install}\label{easy-install}}

\begin{itemize}
\tightlist
\item
  Run \texttt{easy\_install\ -\/-upgrade\ pycm} (Need root access)
\end{itemize}

    \hypertarget{usage}{%
\subsection{Usage}\label{usage}}

    \hypertarget{from-vector}{%
\subsubsection{From Vector}\label{from-vector}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{k+kn}{from} \PY{n+nn}{pycm} \PY{k}{import} \PY{o}{*}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{n}{y\PYZus{}actu} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}
        \PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{n}{cm} \PY{o}{=} \PY{n}{ConfusionMatrix}\PY{p}{(}\PY{n}{y\PYZus{}actu}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{,}\PY{n}{digit}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{)}
\end{Verbatim}


    {Notice } : \texttt{digit} (the number of digits to the right of the
decimal point in a number) is new in {version 0.6} (default value : 5)

Only for print and save

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{n}{cm}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}4}]:} pycm.ConfusionMatrix(classes: [0, 1, 2])
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{actual\PYZus{}vector}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}5}]:} [2, 0, 2, 2, 0, 1, 1, 2, 2, 0, 1, 2]
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{predict\PYZus{}vector}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}6}]:} [0, 0, 2, 1, 0, 2, 1, 0, 2, 0, 2, 2]
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{classes}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}7}]:} [0, 1, 2]
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{class\PYZus{}stat}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}8}]:} \{'ACC': \{0: 0.8333333333333334, 1: 0.75, 2: 0.5833333333333334\},
         'AUC': \{0: 0.8888888888888888, 1: 0.611111111111111, 2: 0.5833333333333333\},
         'BM': \{0: 0.7777777777777777, 1: 0.2222222222222221, 2: 0.16666666666666652\},
         'CEN': \{0: 0.25, 1: 0.49657842846620864, 2: 0.6044162769630221\},
         'DOR': \{0: 'None', 1: 3.999999999999998, 2: 1.9999999999999998\},
         'DP': \{0: 'None', 1: 0.331933069996499, 2: 0.16596653499824957\},
         'DPI': \{0: 'None', 1: 'Poor', 2: 'Poor'\},
         'ERR': \{0: 0.16666666666666663, 1: 0.25, 2: 0.41666666666666663\},
         'F0.5': \{0: 0.6521739130434783,
          1: 0.45454545454545453,
          2: 0.5769230769230769\},
         'F1': \{0: 0.75, 1: 0.4, 2: 0.5454545454545454\},
         'F2': \{0: 0.8823529411764706, 1: 0.35714285714285715, 2: 0.5172413793103449\},
         'FDR': \{0: 0.4, 1: 0.5, 2: 0.4\},
         'FN': \{0: 0, 1: 2, 2: 3\},
         'FNR': \{0: 0.0, 1: 0.6666666666666667, 2: 0.5\},
         'FOR': \{0: 0.0, 1: 0.19999999999999996, 2: 0.4285714285714286\},
         'FP': \{0: 2, 1: 1, 2: 2\},
         'FPR': \{0: 0.2222222222222222,
          1: 0.11111111111111116,
          2: 0.33333333333333337\},
         'G': \{0: 0.7745966692414834, 1: 0.408248290463863, 2: 0.5477225575051661\},
         'IS': \{0: 1.263034405833794, 1: 1.0, 2: 0.2630344058337938\},
         'J': \{0: 0.6, 1: 0.25, 2: 0.375\},
         'LR+': \{0: 4.5, 1: 2.9999999999999987, 2: 1.4999999999999998\},
         'LR-': \{0: 0.0, 1: 0.7500000000000001, 2: 0.75\},
         'MCC': \{0: 0.6831300510639732, 1: 0.25819888974716115, 2: 0.1690308509457033\},
         'MCEN': \{0: 0.2643856189774724, 1: 0.5, 2: 0.6875\},
         'MK': \{0: 0.6000000000000001, 1: 0.30000000000000004, 2: 0.17142857142857126\},
         'N': \{0: 9, 1: 9, 2: 6\},
         'NPV': \{0: 1.0, 1: 0.8, 2: 0.5714285714285714\},
         'P': \{0: 3, 1: 3, 2: 6\},
         'PLRI': \{0: 'Poor', 1: 'Poor', 2: 'Poor'\},
         'POP': \{0: 12, 1: 12, 2: 12\},
         'PPV': \{0: 0.6, 1: 0.5, 2: 0.6\},
         'PRE': \{0: 0.25, 1: 0.25, 2: 0.5\},
         'RACC': \{0: 0.10416666666666667,
          1: 0.041666666666666664,
          2: 0.20833333333333334\},
         'RACCU': \{0: 0.1111111111111111,
          1: 0.04340277777777778,
          2: 0.21006944444444442\},
         'TN': \{0: 7, 1: 8, 2: 4\},
         'TNR': \{0: 0.7777777777777778, 1: 0.8888888888888888, 2: 0.6666666666666666\},
         'TON': \{0: 7, 1: 10, 2: 7\},
         'TOP': \{0: 5, 1: 2, 2: 5\},
         'TP': \{0: 3, 1: 1, 2: 3\},
         'TPR': \{0: 1.0, 1: 0.3333333333333333, 2: 0.5\},
         'Y': \{0: 0.7777777777777777, 1: 0.2222222222222221, 2: 0.16666666666666652\},
         'dInd': \{0: 0.2222222222222222, 1: 0.6758625033664689, 2: 0.6009252125773316\},
         'sInd': \{0: 0.8428651597363228, 1: 0.5220930407198541, 2: 0.5750817072006014\}\}
\end{Verbatim}
            
    {Notice } : \texttt{cm.statistic\_result} prev {versions
0.2\textgreater{}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{overall\PYZus{}stat}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}9}]:} \{'95\% CI': (0.30438856248221097, 0.8622781041844558),
         'AUNP': 0.6666666666666666,
         'AUNU': 0.6944444444444443,
         'Bennett\_S': 0.37500000000000006,
         'CBA': 0.4777777777777778,
         'Chi-Squared': 6.6,
         'Chi-Squared DF': 4,
         'Conditional Entropy': 0.9591479170272448,
         'Cramer\_V': 0.5244044240850757,
         'Cross Entropy': 1.5935164295556343,
         'Gwet\_AC1': 0.3893129770992367,
         'Hamming Loss': 0.41666666666666663,
         'Joint Entropy': 2.4591479170272446,
         'KL Divergence': 0.09351642955563438,
         'Kappa': 0.35483870967741943,
         'Kappa 95\% CI': (-0.07707577422109269, 0.7867531935759315),
         'Kappa No Prevalence': 0.16666666666666674,
         'Kappa Standard Error': 0.2203645326012817,
         'Kappa Unbiased': 0.34426229508196726,
         'Lambda A': 0.16666666666666666,
         'Lambda B': 0.42857142857142855,
         'Mutual Information': 0.5242078379544426,
         'NIR': 0.5,
         'Overall\_ACC': 0.5833333333333334,
         'Overall\_CEN': 0.4638112995385119,
         'Overall\_J': (1.225, 0.4083333333333334),
         'Overall\_MCC': 0.36666666666666664,
         'Overall\_MCEN': 0.5189369467580801,
         'Overall\_RACC': 0.3541666666666667,
         'Overall\_RACCU': 0.3645833333333333,
         'P-Value': 0.38720703125,
         'PPV\_Macro': 0.5666666666666668,
         'PPV\_Micro': 0.5833333333333334,
         'Phi-Squared': 0.5499999999999999,
         'RCI': 0.3494718919696284,
         'RR': 4.0,
         'Reference Entropy': 1.5,
         'Response Entropy': 1.4833557549816874,
         'Scott\_PI': 0.34426229508196726,
         'Standard Error': 0.14231876063832777,
         'Strength\_Of\_Agreement(Altman)': 'Fair',
         'Strength\_Of\_Agreement(Cicchetti)': 'Poor',
         'Strength\_Of\_Agreement(Fleiss)': 'Poor',
         'Strength\_Of\_Agreement(Landis and Koch)': 'Fair',
         'TPR\_Macro': 0.611111111111111,
         'TPR\_Micro': 0.5833333333333334,
         'Zero-one Loss': 5\}
\end{Verbatim}
            
    {Notice } : new in {version 0.3}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{table}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}10}]:} \{0: \{0: 3, 1: 0, 2: 0\}, 1: \{0: 0, 1: 1, 2: 2\}, 2: \{0: 2, 1: 1, 2: 3\}\}
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{matrix}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}11}]:} \{0: \{0: 3, 1: 0, 2: 0\}, 1: \{0: 0, 1: 1, 2: 2\}, 2: \{0: 2, 1: 1, 2: 3\}\}
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{normalized\PYZus{}matrix}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}12}]:} \{0: \{0: 1.0, 1: 0.0, 2: 0.0\},
          1: \{0: 0.0, 1: 0.33333, 2: 0.66667\},
          2: \{0: 0.33333, 1: 0.16667, 2: 0.5\}\}
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{normalized\PYZus{}table}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}13}]:} \{0: \{0: 1.0, 1: 0.0, 2: 0.0\},
          1: \{0: 0.0, 1: 0.33333, 2: 0.66667\},
          2: \{0: 0.33333, 1: 0.16667, 2: 0.5\}\}
\end{Verbatim}
            
    {Notice } : \texttt{matrix}, \texttt{normalized\_matrix} \&
\texttt{normalized\_table} added in {version 1.5} (changed from print
style)

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{k+kn}{import} \PY{n+nn}{numpy}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{n}{y\PYZus{}actu} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
         \PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{numpy}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{n}{cm} \PY{o}{=} \PY{n}{ConfusionMatrix}\PY{p}{(}\PY{n}{y\PYZus{}actu}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{,}\PY{n}{digit}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} \PY{n}{cm}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}17}]:} pycm.ConfusionMatrix(classes: [0, 1, 2])
\end{Verbatim}
            
    {Notice } : \texttt{numpy.array} support in {versions \textgreater{}
0.7}

    \hypertarget{direct-cm}{%
\subsubsection{Direct CM}\label{direct-cm}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{n}{cm2} \PY{o}{=} \PY{n}{ConfusionMatrix}\PY{p}{(}\PY{n}{matrix}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+m+mi}{0}\PY{p}{:} \PY{p}{\PYZob{}}\PY{l+m+mi}{0}\PY{p}{:} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{:} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{:} \PY{l+m+mi}{0}\PY{p}{\PYZcb{}}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{:} \PY{p}{\PYZob{}}\PY{l+m+mi}{0}\PY{p}{:} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{:} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{:} \PY{l+m+mi}{2}\PY{p}{\PYZcb{}}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{:} \PY{p}{\PYZob{}}\PY{l+m+mi}{0}\PY{p}{:} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{:} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{:} \PY{l+m+mi}{3}\PY{p}{\PYZcb{}}\PY{p}{\PYZcb{}}\PY{p}{,}\PY{n}{digit}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{n}{cm2}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}19}]:} pycm.ConfusionMatrix(classes: [0, 1, 2])
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} \PY{n}{cm2}\PY{o}{.}\PY{n}{actual\PYZus{}vector}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} \PY{n}{cm2}\PY{o}{.}\PY{n}{predict\PYZus{}vector}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}22}]:} \PY{n}{cm2}\PY{o}{.}\PY{n}{classes}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}22}]:} [0, 1, 2]
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}23}]:} \PY{n}{cm2}\PY{o}{.}\PY{n}{class\PYZus{}stat}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}23}]:} \{'ACC': \{0: 0.8333333333333334, 1: 0.75, 2: 0.5833333333333334\},
          'AUC': \{0: 0.8888888888888888, 1: 0.611111111111111, 2: 0.5833333333333333\},
          'BM': \{0: 0.7777777777777777, 1: 0.2222222222222221, 2: 0.16666666666666652\},
          'CEN': \{0: 0.25, 1: 0.49657842846620864, 2: 0.6044162769630221\},
          'DOR': \{0: 'None', 1: 3.999999999999998, 2: 1.9999999999999998\},
          'DP': \{0: 'None', 1: 0.331933069996499, 2: 0.16596653499824957\},
          'DPI': \{0: 'None', 1: 'Poor', 2: 'Poor'\},
          'ERR': \{0: 0.16666666666666663, 1: 0.25, 2: 0.41666666666666663\},
          'F0.5': \{0: 0.6521739130434783,
           1: 0.45454545454545453,
           2: 0.5769230769230769\},
          'F1': \{0: 0.75, 1: 0.4, 2: 0.5454545454545454\},
          'F2': \{0: 0.8823529411764706, 1: 0.35714285714285715, 2: 0.5172413793103449\},
          'FDR': \{0: 0.4, 1: 0.5, 2: 0.4\},
          'FN': \{0: 0, 1: 2, 2: 3\},
          'FNR': \{0: 0.0, 1: 0.6666666666666667, 2: 0.5\},
          'FOR': \{0: 0.0, 1: 0.19999999999999996, 2: 0.4285714285714286\},
          'FP': \{0: 2, 1: 1, 2: 2\},
          'FPR': \{0: 0.2222222222222222,
           1: 0.11111111111111116,
           2: 0.33333333333333337\},
          'G': \{0: 0.7745966692414834, 1: 0.408248290463863, 2: 0.5477225575051661\},
          'IS': \{0: 1.263034405833794, 1: 1.0, 2: 0.2630344058337938\},
          'J': \{0: 0.6, 1: 0.25, 2: 0.375\},
          'LR+': \{0: 4.5, 1: 2.9999999999999987, 2: 1.4999999999999998\},
          'LR-': \{0: 0.0, 1: 0.7500000000000001, 2: 0.75\},
          'MCC': \{0: 0.6831300510639732, 1: 0.25819888974716115, 2: 0.1690308509457033\},
          'MCEN': \{0: 0.2643856189774724, 1: 0.5, 2: 0.6875\},
          'MK': \{0: 0.6000000000000001, 1: 0.30000000000000004, 2: 0.17142857142857126\},
          'N': \{0: 9, 1: 9, 2: 6\},
          'NPV': \{0: 1.0, 1: 0.8, 2: 0.5714285714285714\},
          'P': \{0: 3, 1: 3, 2: 6\},
          'PLRI': \{0: 'Poor', 1: 'Poor', 2: 'Poor'\},
          'POP': \{0: 12, 1: 12, 2: 12\},
          'PPV': \{0: 0.6, 1: 0.5, 2: 0.6\},
          'PRE': \{0: 0.25, 1: 0.25, 2: 0.5\},
          'RACC': \{0: 0.10416666666666667,
           1: 0.041666666666666664,
           2: 0.20833333333333334\},
          'RACCU': \{0: 0.1111111111111111,
           1: 0.04340277777777778,
           2: 0.21006944444444442\},
          'TN': \{0: 7, 1: 8, 2: 4\},
          'TNR': \{0: 0.7777777777777778, 1: 0.8888888888888888, 2: 0.6666666666666666\},
          'TON': \{0: 7, 1: 10, 2: 7\},
          'TOP': \{0: 5, 1: 2, 2: 5\},
          'TP': \{0: 3, 1: 1, 2: 3\},
          'TPR': \{0: 1.0, 1: 0.3333333333333333, 2: 0.5\},
          'Y': \{0: 0.7777777777777777, 1: 0.2222222222222221, 2: 0.16666666666666652\},
          'dInd': \{0: 0.2222222222222222, 1: 0.6758625033664689, 2: 0.6009252125773316\},
          'sInd': \{0: 0.8428651597363228, 1: 0.5220930407198541, 2: 0.5750817072006014\}\}
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}24}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{overall\PYZus{}stat}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}24}]:} \{'95\% CI': (0.30438856248221097, 0.8622781041844558),
          'AUNP': 0.6666666666666666,
          'AUNU': 0.6944444444444443,
          'Bennett\_S': 0.37500000000000006,
          'CBA': 0.4777777777777778,
          'Chi-Squared': 6.6,
          'Chi-Squared DF': 4,
          'Conditional Entropy': 0.9591479170272448,
          'Cramer\_V': 0.5244044240850757,
          'Cross Entropy': 1.5935164295556343,
          'Gwet\_AC1': 0.3893129770992367,
          'Hamming Loss': 0.41666666666666663,
          'Joint Entropy': 2.4591479170272446,
          'KL Divergence': 0.09351642955563438,
          'Kappa': 0.35483870967741943,
          'Kappa 95\% CI': (-0.07707577422109269, 0.7867531935759315),
          'Kappa No Prevalence': 0.16666666666666674,
          'Kappa Standard Error': 0.2203645326012817,
          'Kappa Unbiased': 0.34426229508196726,
          'Lambda A': 0.16666666666666666,
          'Lambda B': 0.42857142857142855,
          'Mutual Information': 0.5242078379544426,
          'NIR': 0.5,
          'Overall\_ACC': 0.5833333333333334,
          'Overall\_CEN': 0.4638112995385119,
          'Overall\_J': (1.225, 0.4083333333333334),
          'Overall\_MCC': 0.36666666666666664,
          'Overall\_MCEN': 0.5189369467580801,
          'Overall\_RACC': 0.3541666666666667,
          'Overall\_RACCU': 0.3645833333333333,
          'P-Value': 0.38720703125,
          'PPV\_Macro': 0.5666666666666668,
          'PPV\_Micro': 0.5833333333333334,
          'Phi-Squared': 0.5499999999999999,
          'RCI': 0.3494718919696284,
          'RR': 4.0,
          'Reference Entropy': 1.5,
          'Response Entropy': 1.4833557549816874,
          'Scott\_PI': 0.34426229508196726,
          'Standard Error': 0.14231876063832777,
          'Strength\_Of\_Agreement(Altman)': 'Fair',
          'Strength\_Of\_Agreement(Cicchetti)': 'Poor',
          'Strength\_Of\_Agreement(Fleiss)': 'Poor',
          'Strength\_Of\_Agreement(Landis and Koch)': 'Fair',
          'TPR\_Macro': 0.611111111111111,
          'TPR\_Micro': 0.5833333333333334,
          'Zero-one Loss': 5\}
\end{Verbatim}
            
    {Notice } : new in {version 0.8.1}

In direct matrix mode \texttt{actual\_vector} and
\texttt{predict\_vector} are empty

    \hypertarget{activation-threshold}{%
\subsubsection{Activation Threshold}\label{activation-threshold}}

    \texttt{threshold} is added in \texttt{version\ 0.9} for real value
prediction.

For more information visit
\href{http://www.shaghighi.ir/pycm/doc/Example3.html}{Example3}

    {Notice } : new in {version 0.9}

    \hypertarget{load-from-file}{%
\subsubsection{Load From File}\label{load-from-file}}

    \texttt{file} is added in \texttt{version\ 0.9.5} in order to load saved
confusion matrix with \texttt{.obj} format generated by
\texttt{save\_obj} method.

For more information visit
\href{http://www.shaghighi.ir/pycm/doc/Example4.html}{Example4}

    {Notice } : new in {version 0.9.5}

    \hypertarget{sample-weights}{%
\subsubsection{Sample Weights}\label{sample-weights}}

    \texttt{sample\_weight} is added in \texttt{version\ 1.2}

For more information visit
\href{http://www.shaghighi.ir/pycm/doc/Example5.html}{Example5}

    {Notice } : new in {version 1.2}

    \hypertarget{transpose}{%
\subsubsection{Transpose}\label{transpose}}

    \texttt{transpose} is added in \texttt{version\ 1.2} in order to
transpose input matrix (only in \texttt{Direct\ CM} mode)

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}25}]:} \PY{n}{cm} \PY{o}{=} \PY{n}{ConfusionMatrix}\PY{p}{(}\PY{n}{matrix}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+m+mi}{0}\PY{p}{:} \PY{p}{\PYZob{}}\PY{l+m+mi}{0}\PY{p}{:} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{:} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{:} \PY{l+m+mi}{0}\PY{p}{\PYZcb{}}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{:} \PY{p}{\PYZob{}}\PY{l+m+mi}{0}\PY{p}{:} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{:} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{:} \PY{l+m+mi}{2}\PY{p}{\PYZcb{}}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{:} \PY{p}{\PYZob{}}\PY{l+m+mi}{0}\PY{p}{:} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{:} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{:} \PY{l+m+mi}{3}\PY{p}{\PYZcb{}}\PY{p}{\PYZcb{}}\PY{p}{,}\PY{n}{digit}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,}\PY{n}{transpose}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}26}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{print\PYZus{}matrix}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Predict          0    1    2    
Actual
0                3    0    2    

1                0    1    1    

2                0    2    3    



    \end{Verbatim}

    {Notice } : new in {version 1.2}

    \hypertarget{relabel}{%
\subsubsection{Relabel}\label{relabel}}

    \texttt{relabel} method is added in \texttt{version\ 1.5} in order to
change ConfusionMatrix classnames.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}27}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{relabel}\PY{p}{(}\PY{n}{mapping}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{L1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{:}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{L2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{:}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{L3}\PY{l+s+s2}{\PYZdq{}}\PY{p}{\PYZcb{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}28}]:} \PY{n}{cm}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}28}]:} pycm.ConfusionMatrix(classes: ['L1', 'L2', 'L3'])
\end{Verbatim}
            
    {Notice } : new in {version 1.5}

    \hypertarget{online-help}{%
\subsubsection{Online Help}\label{online-help}}

    \texttt{online\_help} function is added in \texttt{version\ 1.1} in
order to open each statistics definition in web browser

    \begin{Shaded}
\begin{Highlighting}[]

\OperatorTok{>>>} \ImportTok{from}\NormalTok{ pycm }\ImportTok{import}\NormalTok{ online_help}
\OperatorTok{>>>}\NormalTok{ online_help(}\StringTok{"J"}\NormalTok{)}
\OperatorTok{>>>}\NormalTok{ online_help(}\StringTok{"Strength_Of_Agreement(Landis and Koch)"}\NormalTok{)}
\OperatorTok{>>>}\NormalTok{ online_help(}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

    \begin{itemize}
\tightlist
\item
  list of items are available by calling \texttt{online\_help()}
  (without argument)
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}29}]:} \PY{n}{online\PYZus{}help}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Please choose one parameter : 

Example : online\_help("J") or online\_help(2)

1-95\% CI
2-ACC
3-AUC
4-AUNP
5-AUNU
6-BM
7-Bennett\_S
8-CBA
9-CEN
10-Chi-Squared
11-Chi-Squared DF
12-Conditional Entropy
13-Cramer\_V
14-Cross Entropy
15-DOR
16-DP
17-DPI
18-ERR
19-F0.5
20-F1
21-F2
22-FDR
23-FN
24-FNR
25-FOR
26-FP
27-FPR
28-G
29-Gwet\_AC1
30-Hamming Loss
31-IS
32-J
33-Joint Entropy
34-KL Divergence
35-Kappa
36-Kappa 95\% CI
37-Kappa No Prevalence
38-Kappa Standard Error
39-Kappa Unbiased
40-LR+
41-LR-
42-Lambda A
43-Lambda B
44-MCC
45-MCEN
46-MK
47-Mutual Information
48-N
49-NIR
50-NPV
51-Overall\_ACC
52-Overall\_CEN
53-Overall\_J
54-Overall\_MCC
55-Overall\_MCEN
56-Overall\_RACC
57-Overall\_RACCU
58-P
59-P-Value
60-PLRI
61-POP
62-PPV
63-PPV\_Macro
64-PPV\_Micro
65-PRE
66-Phi-Squared
67-RACC
68-RACCU
69-RCI
70-RR
71-Reference Entropy
72-Response Entropy
73-Scott\_PI
74-Standard Error
75-Strength\_Of\_Agreement(Altman)
76-Strength\_Of\_Agreement(Cicchetti)
77-Strength\_Of\_Agreement(Fleiss)
78-Strength\_Of\_Agreement(Landis and Koch)
79-TN
80-TNR
81-TON
82-TOP
83-TP
84-TPR
85-TPR\_Macro
86-TPR\_Micro
87-Y
88-Zero-one Loss
89-dInd
90-sInd

    \end{Verbatim}

    \hypertarget{acceptable-data-types}{%
\subsubsection{Acceptable Data Types}\label{acceptable-data-types}}

    \begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{actual\_vector} : python \texttt{list} or numpy \texttt{array}
  of any stringable objects
\item
  \texttt{predict\_vector} : python \texttt{list} or numpy
  \texttt{array} of any stringable objects
\item
  \texttt{matrix} : \texttt{dict}
\item
  \texttt{digit}: \texttt{int}
\item
  \texttt{threshold} : \texttt{FunctionType\ (function\ or\ lambda)}
\item
  \texttt{file} : \texttt{File\ object}
\item
  \texttt{sample\_weight} : python \texttt{list} or numpy \texttt{array}
  of any stringable objects
\item
  \texttt{transpose} : \texttt{bool}
\end{enumerate}

    \begin{itemize}
\tightlist
\item
  run \texttt{help(ConfusionMatrix)} for more information
\end{itemize}

    \hypertarget{basic-parameters}{%
\subsection{Basic Parameters}\label{basic-parameters}}

    \hypertarget{tp-true-positive-hit}{%
\subsubsection{TP (True positive / hit)}\label{tp-true-positive-hit}}

    A true positive test result is one that detects the condition when the
condition is present. (correctly identified) Section \ref{references}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}30}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{TP}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}30}]:} \{'L1': 3, 'L2': 1, 'L3': 3\}
\end{Verbatim}
            
    \hypertarget{tn-true-negativecorrect-rejection}{%
\subsubsection{TN (True negative/correct
rejection)}\label{tn-true-negativecorrect-rejection}}

    A true negative test result is one that does not detect the condition
when the condition is absent. (correctly rejected)
Section \ref{references}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}31}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{TN}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}31}]:} \{'L1': 7, 'L2': 8, 'L3': 4\}
\end{Verbatim}
            
    \hypertarget{fp-false-positivefalse-alarmtype-i-error}{%
\subsubsection{FP (False positive/false alarm/Type I
error)}\label{fp-false-positivefalse-alarmtype-i-error}}

    A false positive test result is one that detects the condition when the
condition is absent. (incorrectly identified) Section \ref{references}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}32}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{FP}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}32}]:} \{'L1': 0, 'L2': 2, 'L3': 3\}
\end{Verbatim}
            
    \hypertarget{fn-false-negativemisstype-ii-error}{%
\subsubsection{FN (False negative/miss/Type II
error)}\label{fn-false-negativemisstype-ii-error}}

    A false negative test result is one that does not detect the condition
when the condition is present. (incorrectly rejected)
Section \ref{references}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}33}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{FN}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}33}]:} \{'L1': 2, 'L2': 1, 'L3': 2\}
\end{Verbatim}
            
    \hypertarget{p-condition-positive}{%
\subsubsection{P (Condition positive)}\label{p-condition-positive}}

    (number of) positive samples. Also known as support (the number of
occurrences of each class in y\_true) Section \ref{references}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}34}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{P}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}34}]:} \{'L1': 5, 'L2': 2, 'L3': 5\}
\end{Verbatim}
            
    \hypertarget{n-condition-negative}{%
\subsubsection{N (Condition negative)}\label{n-condition-negative}}

    (number of) negative samples Section \ref{references}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}35}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{N}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}35}]:} \{'L1': 7, 'L2': 10, 'L3': 7\}
\end{Verbatim}
            
    \hypertarget{top-test-outcome-positive}{%
\subsubsection{TOP (Test outcome
positive)}\label{top-test-outcome-positive}}

    (number of) positive outcomes Section \ref{references}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}36}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{TOP}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}36}]:} \{'L1': 3, 'L2': 3, 'L3': 6\}
\end{Verbatim}
            
    \hypertarget{ton-test-outcome-negative}{%
\subsubsection{TON (Test outcome
negative)}\label{ton-test-outcome-negative}}

    (number of) negative outcomes Section \ref{references}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}37}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{TON}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}37}]:} \{'L1': 9, 'L2': 9, 'L3': 6\}
\end{Verbatim}
            
    \hypertarget{pop-population}{%
\subsubsection{POP (Population)}\label{pop-population}}

    For more information visit Section \ref{references}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}38}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{POP}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}38}]:} \{'L1': 12, 'L2': 12, 'L3': 12\}
\end{Verbatim}
            
    \begin{itemize}
\tightlist
\item
  Wikipedia page
\end{itemize}

    \hypertarget{class-statistics}{%
\subsection{Class Statistics}\label{class-statistics}}

    \hypertarget{tpr-sensitivity-recall-hit-rate-or-true-positive-rate}{%
\subsubsection{TPR (sensitivity, recall, hit rate, or true positive
rate)}\label{tpr-sensitivity-recall-hit-rate-or-true-positive-rate}}

    Sensitivity (also called the true positive rate, the recall, or
probability of detection in some fields) measures the proportion of
positives that are correctly identified as such (e.g.~the percentage of
sick people who are correctly identified as having the condition).
Section \ref{references}

Wikipedia page

    \[TPR=\frac{TP}{P}=\frac{TP}{TP+FN}\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}39}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{TPR}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}39}]:} \{'L1': 0.6, 'L2': 0.5, 'L3': 0.6\}
\end{Verbatim}
            
    \hypertarget{tnr-specificity-or-true-negative-rate}{%
\subsubsection{TNR (specificity or true negative
rate)}\label{tnr-specificity-or-true-negative-rate}}

    Specificity (also called the true negative rate) measures the proportion
of negatives that are correctly identified as such (e.g.~the percentage
of healthy people who are correctly identified as not having the
condition). Section \ref{references}

Wikipedia page

    \[TNR=\frac{TN}{N}=\frac{TN}{TN+FP}\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}40}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{TNR}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}40}]:} \{'L1': 1.0, 'L2': 0.8, 'L3': 0.5714285714285714\}
\end{Verbatim}
            
    \hypertarget{ppv-precision-or-positive-predictive-value}{%
\subsubsection{PPV (precision or positive predictive
value)}\label{ppv-precision-or-positive-predictive-value}}

    Predictive value positive is the proportion of positives that correspond
to the presence of the condition. Section \ref{references}

Wikipedia page

    \[PPV=\frac{TP}{TP+FP}\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}41}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{PPV}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}41}]:} \{'L1': 1.0, 'L2': 0.3333333333333333, 'L3': 0.5\}
\end{Verbatim}
            
    \hypertarget{npv-negative-predictive-value}{%
\subsubsection{NPV (negative predictive
value)}\label{npv-negative-predictive-value}}

    Predictive value negative is the proportion of negatives that correspond
to the absence of the condition. Section \ref{references}

Wikipedia page

    \[NPV=\frac{TN}{TN+FN}\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}42}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{NPV}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}42}]:} \{'L1': 0.7777777777777778, 'L2': 0.8888888888888888, 'L3': 0.6666666666666666\}
\end{Verbatim}
            
    \hypertarget{fnr-miss-rate-or-false-negative-rate}{%
\subsubsection{FNR (miss rate or false negative
rate)}\label{fnr-miss-rate-or-false-negative-rate}}

    The false negative rate is the proportion of positives which yield
negative test outcomes with the test, i.e., the conditional probability
of a negative test result given that the condition being looked for is
present. Section \ref{references}

Wikipedia page

    \[FNR=\frac{FN}{P}=\frac{FN}{FN+TP}=1-TPR\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}43}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{FNR}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}43}]:} \{'L1': 0.4, 'L2': 0.5, 'L3': 0.4\}
\end{Verbatim}
            
    \hypertarget{fpr-fall-out-or-false-positive-rate}{%
\subsubsection{FPR (fall-out or false positive
rate)}\label{fpr-fall-out-or-false-positive-rate}}

    The false positive rate is the proportion of all negatives that still
yield positive test outcomes, i.e., the conditional probability of a
positive test result given an event that was not present.
Section \ref{references}

The false positive rate is equal to the significance level. The
specificity of the test is equal to 1 minus the false positive rate.

Wikipedia page

    \[FPR=\frac{FP}{N}=\frac{FP}{FP+TN}=1-TNR\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}44}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{FPR}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}44}]:} \{'L1': 0.0, 'L2': 0.19999999999999996, 'L3': 0.4285714285714286\}
\end{Verbatim}
            
    \hypertarget{fdr-false-discovery-rate}{%
\subsubsection{FDR (false discovery
rate)}\label{fdr-false-discovery-rate}}

    The false discovery rate (FDR) is a method of conceptualizing the rate
of type I errors in null hypothesis testing when conducting multiple
comparisons. FDR-controlling procedures are designed to control the
expected proportion of ``discoveries'' (rejected null hypotheses) that
are false (incorrect rejections). Section \ref{references}

Wikipedia page

    \[FDR=\frac{FP}{FP+TP}=1-PPV\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}45}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{FDR}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}45}]:} \{'L1': 0.0, 'L2': 0.6666666666666667, 'L3': 0.5\}
\end{Verbatim}
            
    \hypertarget{for-false-omission-rate}{%
\subsubsection{FOR (false omission
rate)}\label{for-false-omission-rate}}

    False omission rate (FOR) is a statistical method used in multiple
hypothesis testing to correct for multiple comparisons and it is the
complement of the negative predictive value. It measures the proportion
of false negatives which are incorrectly rejected.
Section \ref{references}

Wikipedia page

    \[FOR=\frac{FN}{FN+TN}=1-NPV\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}46}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{FOR}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}46}]:} \{'L1': 0.2222222222222222,
          'L2': 0.11111111111111116,
          'L3': 0.33333333333333337\}
\end{Verbatim}
            
    \hypertarget{acc-accuracy}{%
\subsubsection{ACC (accuracy)}\label{acc-accuracy}}

    The accuracy is the number of correct predictions from all predictions
made. Section \ref{references}

Wikipedia page

    \[ACC=\frac{TP+TN}{P+N}=\frac{TP+TN}{TP+TN+FP+FN}\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}47}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{ACC}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}47}]:} \{'L1': 0.8333333333333334, 'L2': 0.75, 'L3': 0.5833333333333334\}
\end{Verbatim}
            
    \hypertarget{errerror-rate}{%
\subsubsection{ERR(Error rate)}\label{errerror-rate}}

    The accuracy is the number of incorrect predictions from all predictions
made. Section \ref{references}

    \[ERR=\frac{FP+FN}{P+N}=\frac{FP+FN}{TP+TN+FP+FN}=1-ACC\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}48}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{ERR}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}48}]:} \{'L1': 0.16666666666666663, 'L2': 0.25, 'L3': 0.41666666666666663\}
\end{Verbatim}
            
    {Notice } : new in {version 0.4}

    \hypertarget{fbeta-score}{%
\subsubsection{FBeta-Score}\label{fbeta-score}}

    In statistical analysis of classification, the F1 score (also F-score or
F-measure) is a measure of a test's accuracy. It considers both the
precision p and the recall r of the test to compute the score. The F1
score is the harmonic average of the precision and recall, where an F1
score reaches its best value at 1 (perfect precision and recall) and
worst at 0. Section \ref{references}

Wikipedia page

    \[F_{\beta}=(1+\beta^2).\frac{PPV.TPR}{(\beta^2.PPV)+TPR}=\frac{(1+\beta^2).TP}{(1+\beta^2).TP+FP+\beta^2.FN}\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}49}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{F1}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}49}]:} \{'L1': 0.75, 'L2': 0.4, 'L3': 0.5454545454545454\}
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}50}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{F05}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}50}]:} \{'L1': 0.8823529411764706, 'L2': 0.35714285714285715, 'L3': 0.5172413793103449\}
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}51}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{F2}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}51}]:} \{'L1': 0.6521739130434783, 'L2': 0.45454545454545453, 'L3': 0.5769230769230769\}
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}52}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{F\PYZus{}beta}\PY{p}{(}\PY{n}{Beta}\PY{o}{=}\PY{l+m+mi}{4}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}52}]:} \{'L1': 0.6144578313253012, 'L2': 0.4857142857142857, 'L3': 0.5930232558139535\}
\end{Verbatim}
            
    {Notice } : new in {version 0.4}

    \hypertarget{mcc-matthews-correlation-coefficient}{%
\subsubsection{MCC (Matthews correlation
coefficient)}\label{mcc-matthews-correlation-coefficient}}

    The Matthews correlation coefficient is used in machine learning as a
measure of the quality of binary (two-class) classifications, introduced
by biochemist Brian W. Matthews in 1975. It takes into account true and
false positives and negatives and is generally regarded as a balanced
measure which can be used even if the classes are of very different
sizes.The MCC is in essence a correlation coefficient between the
observed and predicted binary classifications; it returns a value
between −1 and +1. A coefficient of +1 represents a perfect prediction,
0 no better than random prediction and −1 indicates total disagreement
between prediction and observation. Section \ref{references}

Wikipedia page

    \[MCC=\frac{TP \times TN-FP \times FN}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}53}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{MCC}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}53}]:} \{'L1': 0.6831300510639732, 'L2': 0.25819888974716115, 'L3': 0.1690308509457033\}
\end{Verbatim}
            
    \hypertarget{bm-informedness-or-bookmaker-informedness}{%
\subsubsection{BM (Informedness or Bookmaker
Informedness)}\label{bm-informedness-or-bookmaker-informedness}}

    The informedness of a prediction method as captured by a contingency
matrix is defined as the probability that the prediction method will
make a correct decision as opposed to guessing and is calculated using
the bookmaker algorithm. Section \ref{references}

    \[BM=TPR+TNR-1\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}54}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{BM}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}54}]:} \{'L1': 0.6000000000000001,
          'L2': 0.30000000000000004,
          'L3': 0.17142857142857126\}
\end{Verbatim}
            
    \hypertarget{mk-markedness}{%
\subsubsection{MK (Markedness)}\label{mk-markedness}}

    In statistics and psychology, the social science concept of markedness
is quantified as a measure of how much one variable is marked as a
predictor or possible cause of another, and is also known as Δp (deltaP)
in simple two-choice cases. Section \ref{references}

    \[MK=PPV+NPV-1\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}55}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{MK}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}55}]:} \{'L1': 0.7777777777777777, 'L2': 0.2222222222222221, 'L3': 0.16666666666666652\}
\end{Verbatim}
            
    \hypertarget{plr-positive-likelihood-ratio}{%
\subsubsection{PLR (Positive likelihood
ratio)}\label{plr-positive-likelihood-ratio}}

    Likelihood ratios are used for assessing the value of performing a
diagnostic test. They use the sensitivity and specificity of the test to
determine whether a test result usefully changes the probability that a
condition (such as a disease state) exists. The first description of the
use of likelihood ratios for decision rules was made at a symposium on
information theory in 1954. Section \ref{references}

Wikipedia page

    \[LR_+=PLR=\frac{TPR}{FPR}\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}56}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{PLR}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}56}]:} \{'L1': 'None', 'L2': 2.5000000000000004, 'L3': 1.4\}
\end{Verbatim}
            
    {Notice } : \texttt{LR+} renamed to \texttt{PLR} in {version 1.5}

    \hypertarget{nlr-negative-likelihood-ratio}{%
\subsubsection{NLR (Negative likelihood
ratio)}\label{nlr-negative-likelihood-ratio}}

    Likelihood ratios are used for assessing the value of performing a
diagnostic test. They use the sensitivity and specificity of the test to
determine whether a test result usefully changes the probability that a
condition (such as a disease state) exists. The first description of the
use of likelihood ratios for decision rules was made at a symposium on
information theory in 1954. Section \ref{references}

Wikipedia page

    \[LR_-=NLR=\frac{FNR}{TNR}\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}57}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{NLR}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}57}]:} \{'L1': 0.4, 'L2': 0.625, 'L3': 0.7000000000000001\}
\end{Verbatim}
            
    {Notice } : \texttt{LR-} renamed to \texttt{NLR} in {version 1.5}

    \hypertarget{dor-diagnostic-odds-ratio}{%
\subsubsection{DOR (Diagnostic odds
ratio)}\label{dor-diagnostic-odds-ratio}}

    The diagnostic odds ratio is a measure of the effectiveness of a
diagnostic test. It is defined as the ratio of the odds of the test
being positive if the subject has a disease relative to the odds of the
test being positive if the subject does not have the disease.
Section \ref{references}

Wikipedia page

    \[DOR=\frac{LR+}{LR-}\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}58}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{DOR}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}58}]:} \{'L1': 'None', 'L2': 4.000000000000001, 'L3': 1.9999999999999998\}
\end{Verbatim}
            
    \hypertarget{pre-prevalence}{%
\subsubsection{PRE (Prevalence)}\label{pre-prevalence}}

    Prevalence is a statistical concept referring to the number of cases of
a disease that are present in a particular population at a given time
(Reference Likelihood). Section \ref{references}

Wikipedia page

    \[Prevalence=\frac{P}{POP}\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}59}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{PRE}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}59}]:} \{'L1': 0.4166666666666667, 'L2': 0.16666666666666666, 'L3': 0.4166666666666667\}
\end{Verbatim}
            
    \hypertarget{g-g-measure-geometric-mean-of-precision-and-sensitivity}{%
\subsubsection{G (G-measure geometric mean of precision and
sensitivity)}\label{g-g-measure-geometric-mean-of-precision-and-sensitivity}}

    Geometric mean of precision and sensitivity Section \ref{references}

Wikipedia page

    \[G=\sqrt{PPV.TPR}\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}60}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{G}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}60}]:} \{'L1': 0.7745966692414834, 'L2': 0.408248290463863, 'L3': 0.5477225575051661\}
\end{Verbatim}
            
    \hypertarget{raccrandom-accuracy}{%
\subsubsection{RACC(Random accuracy)}\label{raccrandom-accuracy}}

    The expected accuracy from a strategy of randomly guessing categories
according to reference and response distributions.
Section \ref{references}

    \[RACC=\frac{TOP\times P}{POP^2}\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}61}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{RACC}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}61}]:} \{'L1': 0.10416666666666667,
          'L2': 0.041666666666666664,
          'L3': 0.20833333333333334\}
\end{Verbatim}
            
    {Notice } : new in {version 0.3}

    \hypertarget{raccurandom-accuracy-unbiased}{%
\subsubsection{RACCU(Random accuracy
unbiased)}\label{raccurandom-accuracy-unbiased}}

    The expected accuracy from a strategy of randomly guessing categories
according to the average of the reference and response distributions.
Section \ref{references}

    \[RACCU=(\frac{TOP+P}{2\times POP})^2\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}62}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{RACCU}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}62}]:} \{'L1': 0.1111111111111111,
          'L2': 0.04340277777777778,
          'L3': 0.21006944444444442\}
\end{Verbatim}
            
    {Notice } : new in {version 0.8.1}

    \hypertarget{j-jaccard-index}{%
\subsubsection{J (Jaccard index)}\label{j-jaccard-index}}

    The Jaccard index, also known as Intersection over Union and the Jaccard
similarity coefficient (originally coined coefficient de communauté by
Paul Jaccard), is a statistic used for comparing the similarity and
diversity of sample sets. Section \ref{references}

Wikipedia page

    \[J(A,B)=\frac{|A\cap B|}{|A\cup B|}=\frac{|A\cap B|}{|A|+|B|-|A\cap B|}\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}63}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{J}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}63}]:} \{'L1': 0.6, 'L2': 0.25, 'L3': 0.375\}
\end{Verbatim}
            
    {Notice } : new in {version 0.9}

    \hypertarget{is-information-score}{%
\subsubsection{IS (Information Score)}\label{is-information-score}}

    The amount of information needed to correctly classify an example into
class C, whose prior probability is p(C), is defined as -log2(p(C)).
Section \ref{references}

    \[IS=-log_2(\frac{TP+FN}{POP})+log_2(\frac{TP}{TP+FP})\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}64}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{IS}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}64}]:} \{'L1': 1.2630344058337937, 'L2': 0.9999999999999998, 'L3': 0.26303440583379367\}
\end{Verbatim}
            
    {Notice } : new in {version 1.3}

    \hypertarget{cen-confusion-entropy}{%
\subsubsection{CEN (Confusion Entropy)}\label{cen-confusion-entropy}}

    CEN based upon the concept of entropy for evaluating classifier
performances. By exploiting the misclassification information of
confusion matrices, the measure evaluates the confusion level of the
class distribution of misclassified samples. Both theoretical analysis
and statistical results show that the proposed measure is more
discriminating than accuracy and RCI while it remains relatively
consistent with the two measures. Moreover, it is more capable of
measuring how the samples of different classes have been separated from
each other. Hence the proposed measure is more precise than the two
measures and can substitute for them to evaluate classifiers in
classification applications. Section \ref{references}

    \[P_{i,j}^{j}=\frac{Matrix(i,j)}{\sum_{k=1}^{|C|}Matrix(j,k)+Matrix(k,j)}\]

    \[P_{i,j}^{i}=\frac{Matrix(i,j)}{\sum_{k=1}^{|C|}Matrix(i,k)+Matrix(k,i)}\]

    \[CEN_j=-\sum_{k=1,k\neq j}^{|C|}(P_{j,k}^jlog_{2(|C|-1)}(P_{j,k}^j)+P_{k,j}^jlog_{2(|C|-1)}(P_{k,j}^j))\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}65}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{CEN}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}65}]:} \{'L1': 0.25, 'L2': 0.49657842846620864, 'L3': 0.6044162769630221\}
\end{Verbatim}
            
    {Notice } : new in {version 1.3}

    \hypertarget{mcen-modified-confusion-entropy}{%
\subsubsection{MCEN (Modified Confusion
Entropy)}\label{mcen-modified-confusion-entropy}}

    Modified version of CEN Section \ref{references}

    \[P_{i,j}^{j}=\frac{Matrix(i,j)}{\sum_{k=1}^{|C|}(Matrix(j,k)+Matrix(k,j))-Matrix(j,j)}\]

    \[P_{i,j}^{i}=\frac{Matrix(i,j)}{\sum_{k=1}^{|C|}(Matrix(i,k)+Matrix(k,i))-Matrix(i,i)}\]

    \[MCEN_j=-\sum_{k=1,k\neq j}^{|C|}(P_{j,k}^jlog_{2(|C|-1)}(P_{j,k}^j)+P_{k,j}^jlog_{2(|C|-1)}(P_{k,j}^j))\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}66}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{MCEN}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}66}]:} \{'L1': 0.2643856189774724, 'L2': 0.5, 'L3': 0.6875\}
\end{Verbatim}
            
    {Notice } : new in {version 1.3}

    \hypertarget{auc-area-under-the-roc-curve}{%
\subsubsection{AUC (Area Under The ROC
Curve)}\label{auc-area-under-the-roc-curve}}

    Thus, AUC corresponds to the arithmetic mean of sensitivity and
specificity values of each class. Section \ref{references}

    \[AUC=\frac{TNR+TPR}{2}\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}67}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{AUC}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}67}]:} \{'L1': 0.8, 'L2': 0.65, 'L3': 0.5857142857142856\}
\end{Verbatim}
            
    {Notice } : new in {version 1.4}

    \hypertarget{dind-distance-index}{%
\subsubsection{dInd (Distance Index)}\label{dind-distance-index}}

    Euclidean distance of a ROC point from the top left corner of the ROC
space, which can take values between 0 (perfect classification) and
sqrt(2). Section \ref{references}

    \[dInd=\sqrt{(1-TNR)^2+(1-TPR)^2}\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}68}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{dInd}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}68}]:} \{'L1': 0.4, 'L2': 0.5385164807134504, 'L3': 0.5862367008195198\}
\end{Verbatim}
            
    {Notice } : new in {version 1.4}

    \hypertarget{sind-similarity-index}{%
\subsubsection{sInd (Similarity Index)}\label{sind-similarity-index}}

    sInd is comprised between 0 (no correct classifications) and 1 (perfect
classification). Section \ref{references}

    \[sInd = 1 - \sqrt{\frac{(1-TNR)^2+(1-TPR)^2}{2}}\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}69}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{sInd}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}69}]:} \{'L1': 0.717157287525381, 'L2': 0.6192113447068046, 'L3': 0.5854680534700882\}
\end{Verbatim}
            
    {Notice } : new in {version 1.4}

    \hypertarget{dp-discriminant-power}{%
\subsubsection{DP (Discriminant Power)}\label{dp-discriminant-power}}

    Discriminant power (DP) is a measure that summarizes sensitivity and
specificity. The DP has been used mainly in feature selection over
imbalanced data. Section \ref{references}

    \[X=\frac{TPR}{1-TPR}\]

    \[Y=\frac{TNR}{1-TNR}\]

    \[DP=\frac{\sqrt{3}}{\pi}(log_{10}X+log_{10}Y)\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}70}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{DP}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}70}]:} \{'L1': 'None', 'L2': 0.33193306999649924, 'L3': 0.1659665349982495\}
\end{Verbatim}
            
    {Notice } : new in {version 1.5}

    \hypertarget{y-youden-index}{%
\subsubsection{Y (Youden Index)}\label{y-youden-index}}

    Youden's index, evaluates the algorithm's ability to avoid failure; it's
derived from sensitivity and specificity and denotes a linear
correspondence balanced accuracy. as Youden's index is a linear
transformation of the mean sensitivity and specificity, its values are
difficult to interpret , we retain that a higher value of Y indicates
better ability to avoid failure. Youden's index has been conventionally
used to evaluate tests diagnostic , improve efficiency of Telemedical
prevention. Section \ref{references} Section \ref{references}

Wikipedia page

    \[\gamma=BM=TPR+TNR-1\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}71}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{Y}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}71}]:} \{'L1': 0.6000000000000001,
          'L2': 0.30000000000000004,
          'L3': 0.17142857142857126\}
\end{Verbatim}
            
    {Notice } : new in {version 1.5}

    \hypertarget{plri-positive-likelihood-ratio-interpretation}{%
\subsubsection{PLRI (Positive likelihood ratio
interpretation)}\label{plri-positive-likelihood-ratio-interpretation}}

    For more information visit Section \ref{references}

    PLR

Model contribution

1 \textgreater{}

Negligible

1 - 5

Poor

5 - 10

Fair

\begin{quote}
10

Good
\end{quote}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}72}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{PLRI}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}72}]:} \{'L1': 'None', 'L2': 'Poor', 'L3': 'Poor'\}
\end{Verbatim}
            
    {Notice } : new in {version 1.5}

    \hypertarget{dpi-discriminant-power-interpretation}{%
\subsubsection{DPI (Discriminant power
interpretation)}\label{dpi-discriminant-power-interpretation}}

    For more information visit Section \ref{references}

    DP

Model contribution

1 \textgreater{}

Poor

1 - 2

Limited

2 - 3

Fair

\begin{quote}
3

Good
\end{quote}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}73}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{DPI}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}73}]:} \{'L1': 'None', 'L2': 'Poor', 'L3': 'Poor'\}
\end{Verbatim}
            
    {Notice } : new in {version 1.5}

    \hypertarget{overall-statistics}{%
\subsection{Overall Statistics}\label{overall-statistics}}

    \hypertarget{kappa-nominal}{%
\subsubsection{Kappa (Nominal)}\label{kappa-nominal}}

    Kappa is a statistic which measures inter-rater agreement for
qualitative (categorical) items. It is generally thought to be a more
robust measure than simple percent agreement calculation, as kappa takes
into account the possibility of the agreement occurring by chance.
Section \ref{references}

Wikipedia page

    \[Kappa=\frac{ACC_{Overall}-RACC_{Overall}}{1-RACC_{Overall}}\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}74}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{Kappa}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}74}]:} 0.35483870967741943
\end{Verbatim}
            
    {Notice } : new in {version 0.3}

    \hypertarget{kappa-unbiased}{%
\subsubsection{Kappa Unbiased}\label{kappa-unbiased}}

    The unbiased kappa value is defined in terms of total accuracy and a
slightly different computation of expected likelihood that averages the
reference and response probabilities. Section \ref{references}

    \[Kappa_{Unbiased}=\frac{ACC_{Overall}-RACCU_{Overall}}{1-RACCU_{Overall}}\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}75}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{KappaUnbiased}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}75}]:} 0.34426229508196726
\end{Verbatim}
            
    {Notice } : new in {version 0.8.1}

    \hypertarget{kappa-no-prevalence}{%
\subsubsection{Kappa No Prevalence}\label{kappa-no-prevalence}}

    The kappa statistic adjusted for prevalence. Section \ref{references}

    \[Kappa_{NoPrevalence}=2 \times ACC_{Overall}-1\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}76}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{KappaNoPrevalence}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}76}]:} 0.16666666666666674
\end{Verbatim}
            
    {Notice } : new in {version 0.8.1}

    \hypertarget{kappa-95-ci}{%
\subsubsection{Kappa 95\% CI}\label{kappa-95-ci}}

    Kappa 95\% Confidence Interval Section \ref{references}

    \[SE_{Kappa}=\sqrt{\frac{ACC_{Overall}(1-RACC_{Overall})}{(1-RACC_{Overall})^2}}\]

    \[Kappa \pm 1.96\times SE_{Kappa}\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}77}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{Kappa\PYZus{}SE}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}77}]:} 0.2203645326012817
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}78}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{Kappa\PYZus{}CI}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}78}]:} (-0.07707577422109269, 0.7867531935759315)
\end{Verbatim}
            
    {Notice } : new in {version 0.7}

    \hypertarget{chi-squared}{%
\subsubsection{Chi-Squared}\label{chi-squared}}

    Pearson's chi-squared test is a statistical test applied to sets of
categorical data to evaluate how likely it is that any observed
difference between the sets arose by chance. It is suitable for unpaired
data from large samples. Section \ref{references}

Wikipedia page

    \[\chi^2=\sum_{i=1}^n\sum_{j=1}^n\frac{(Matrix(i,j)-E(i,j))^2}{E(i,j)}\]

    \[E(i,j)=\frac{TOP_j\times P_i}{POP}\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}79}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{Chi\PYZus{}Squared}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}79}]:} 6.6000000000000005
\end{Verbatim}
            
    {Notice } : new in {version 0.7}

    \hypertarget{chi-squared-df}{%
\subsubsection{Chi-Squared DF}\label{chi-squared-df}}

    Number of degrees of freedom of this confusion matrix for the
chi-squared statistic. Section \ref{references}

    \[DF=(|C|-1)^2\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}80}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{DF}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}80}]:} 4
\end{Verbatim}
            
    {Notice } : new in {version 0.7}

    \hypertarget{phi-squared}{%
\subsubsection{Phi-Squared}\label{phi-squared}}

    In statistics, the phi coefficient (or mean square contingency
coefficient) is a measure of association for two binary variables.
Introduced by Karl Pearson, this measure is similar to the Pearson
correlation coefficient in its interpretation. In fact, a Pearson
correlation coefficient estimated for two binary variables will return
the phi coefficient. Section \ref{references}

Wikipedia page

    \[\phi^2=\frac{\chi^2}{POP}\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}81}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{Phi\PYZus{}Squared}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}81}]:} 0.55
\end{Verbatim}
            
    {Notice } : new in {version 0.7}

    \hypertarget{cramers-v}{%
\subsubsection{Cramer's V}\label{cramers-v}}

    In statistics, Cramér's V (sometimes referred to as Cramér's phi) is a
measure of association between two nominal variables, giving a value
between 0 and +1 (inclusive). It is based on Pearson's chi-squared
statistic and was published by Harald Cramér in 1946.
Section \ref{references}

Wikipedia page

    \[V=\sqrt{\frac{\phi^2}{|C|-1}}\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}82}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{V}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}82}]:} 0.5244044240850758
\end{Verbatim}
            
    {Notice } : new in {version 0.7}

    \hypertarget{ci}{%
\subsubsection{95\% CI}\label{ci}}

    In statistics, a confidence interval (CI) is a type of interval estimate
(of a population parameter) that is computed from the observed data. The
confidence level is the frequency (i.e., the proportion) of possible
confidence intervals that contain the true value of their corresponding
parameter. In other words, if confidence intervals are constructed using
a given confidence level in an infinite number of independent
experiments, the proportion of those intervals that contain the true
value of the parameter will match the confidence level.
Section \ref{references}

Wikipedia page

    \[SE_{ACC}=\sqrt{\frac{ACC\times (1-ACC)}{POP}}\]

    \[ACC \pm 1.96\times SE_{ACC}\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}83}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{CI}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}83}]:} (0.30438856248221097, 0.8622781041844558)
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}84}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{SE}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}84}]:} 0.14231876063832777
\end{Verbatim}
            
    {Notice } : new in {version 0.7}

    \hypertarget{bennett-et-al.s-s-score-nominal}{%
\subsubsection{Bennett et al.'s S score
(Nominal)}\label{bennett-et-al.s-s-score-nominal}}

    Bennett, Alpert \& Goldstein's S is a statistical measure of inter-rater
agreement. It was created by Bennett et al.~in 1954 Bennett et
al.~suggested adjusting inter-rater reliability to accommodate the
percentage of rater agreement that might be expected by chance was a
better measure than simple agreement between raters.
Section \ref{references}

Wikipedia Page

    \[p_c=\frac{1}{|C|}\]

    \[S=\frac{ACC_{Overall}-p_c}{1-p_c}\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}85}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{S}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}85}]:} 0.37500000000000006
\end{Verbatim}
            
    {Notice } : new in {version 0.5}

    \hypertarget{scotts-pi-nominal}{%
\subsubsection{Scott's pi (Nominal)}\label{scotts-pi-nominal}}

    Scott's pi (named after William A. Scott) is a statistic for measuring
inter-rater reliability for nominal data in communication studies.
Textual entities are annotated with categories by different annotators,
and various measures are used to assess the extent of agreement between
the annotators, one of which is Scott's pi. Since automatically
annotating text is a popular problem in natural language processing, and
goal is to get the computer program that is being developed to agree
with the humans in the annotations it creates, assessing the extent to
which humans agree with each other is important for establishing a
reasonable upper limit on computer performance. Section \ref{references}

Wikipedia page

    \[p_c=\sum_{i=1}^{|C|}(\frac{TOP_i + P_i}{2\times POP})^2\]

    \[\pi=\frac{ACC_{Overall}-p_c}{1-p_c}\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}86}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{PI}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}86}]:} 0.34426229508196726
\end{Verbatim}
            
    {Notice } : new in {version 0.5}

    \hypertarget{gwets-ac1}{%
\subsubsection{Gwet's AC1}\label{gwets-ac1}}

    AC1 was originally introduced by Gwet in 2001 (Gwet, 2001). The
interpretation of AC1 is similar to generalized kappa (Fleiss, 1971),
which is used to assess interrater reliability of when there are
multiple raters. Gwet (2002) demonstrated that AC1 can overcome the
limitations that kappa is sensitive to trait prevalence and rater's
classification probabilities (i.e., marginal probabilities), whereas AC1
provides more robust measure of interrater reliability.
Section \ref{references}

    \[\pi=\frac{TOP_i + P_i}{2\times POP}\]

    \[p_c=\frac{1}{|C|-1}\sum_{i=1}^{|C|}(\pi_i\times (1-\pi_i))\]

    \[AC1=\frac{ACC_{Overall}-p_c}{1-p_c}\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}87}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{AC1}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}87}]:} 0.3893129770992367
\end{Verbatim}
            
    {Notice } : new in {version 0.5}

    \hypertarget{reference-entropy}{%
\subsubsection{Reference Entropy}\label{reference-entropy}}

    The entropy of the decision problem itself as defined by the counts for
the reference. The entropy of a distribution is the average negative log
probability of outcomes. Section \ref{references}

    \[Likelihood_{Reference}=\frac{P_i}{POP}\]

    \[Entropy_{Reference}=-\sum_{i=1}^{|C|}Likelihood_{Reference}(i)\times\log_{2}{Likelihood_{Reference}(i)}\]

    \[0\times\log_{2}{0}\equiv0\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}88}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{ReferenceEntropy}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}88}]:} 1.4833557549816874
\end{Verbatim}
            
    {Notice } : new in {version 0.8.1}

    \hypertarget{response-entropy}{%
\subsubsection{Response Entropy}\label{response-entropy}}

    The entropy of the response distribution. The entropy of a distribution
is the average negative log probability of outcomes.
Section \ref{references}

    \[Likelihood_{Response}=\frac{TOP_i}{POP}\]

    \[Entropy_{Response}=-\sum_{i=1}^{|C|}Likelihood_{Response}(i)\times\log_{2}{Likelihood_{Response}(i)}\]

    \[0\times\log_{2}{0}\equiv0\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}89}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{ResponseEntropy}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}89}]:} 1.5
\end{Verbatim}
            
    {Notice } : new in {version 0.8.1}

    \hypertarget{cross-entropy}{%
\subsubsection{Cross Entropy}\label{cross-entropy}}

    The cross-entropy of the response distribution against the reference
distribution. The cross-entropy is defined by the negative log
probabilities of the response distribution weighted by the reference
distribution. Section \ref{references}

Wikipedia page

    \[Likelihood_{Reference}=\frac{P_i}{POP}\]

    \[Likelihood_{Response}=\frac{TOP_i}{POP}\]

    \[Entropy_{Cross}=-\sum_{i=1}^{|C|}Likelihood_{Reference}(i)\times\log_{2}{Likelihood_{Response}(i)}\]

    \[0\times\log_{2}{0}\equiv0\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}90}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{CrossEntropy}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}90}]:} 1.5833333333333335
\end{Verbatim}
            
    {Notice } : new in {version 0.8.1}

    \hypertarget{joint-entropy}{%
\subsubsection{Joint Entropy}\label{joint-entropy}}

    The entropy of the joint reference and response distribution as defined
by the underlying matrix. Section \ref{references}

    \[P^{'}(i,j)=\frac{Matrix(i,j)}{POP}\]

    \[Entropy_{Joint}=-\sum_{i=1}^{|C|}\sum_{j=1}^{|C|}P^{'}(i,j)\times\log_{2}{P^{'}(i,j)}\]

    \[0\times\log_{2}{0}\equiv0\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}91}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{JointEntropy}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}91}]:} 2.4591479170272446
\end{Verbatim}
            
    {Notice } : new in {version 0.8.1}

    \hypertarget{conditional-entropy}{%
\subsubsection{Conditional Entropy}\label{conditional-entropy}}

    The entropy of the distribution of categories in the response given that
the reference category was as specified. Section \ref{references}

Wikipedia page

    \[P^{'}(j|i)=\frac{Matrix(j,i)}{P_i}\]

    \[\sum_{i=1}^{|C|}(Likelihood_{Reference}(i)\times(-\sum_{j=1}^{|C|}P^{'}(j|i)\times\log_{2}{P^{'}(j|i)}))\]

    \[0\times\log_{2}{0}\equiv0\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}92}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{ConditionalEntropy}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}92}]:} 0.9757921620455572
\end{Verbatim}
            
    {Notice } : new in {version 0.8.1}

    \hypertarget{kullback-liebler-kl-divergence}{%
\subsubsection{Kullback-Liebler (KL)
divergence}\label{kullback-liebler-kl-divergence}}

    In mathematical statistics, the Kullback--Leibler divergence (also
called relative entropy) is a measure of how one probability
distribution diverges from a second, expected probability distribution.
Section \ref{references} Section \ref{references}

Wikipedia Page

    \[Likelihood_{Response}=\frac{TOP_i}{POP}\]

    \[Likelihood_{Reference}=\frac{P_i}{POP}\]

    \[Divergence=-\sum_{i=1}^{|C|}Likelihood_{Reference}\times\log_{2}{\frac{Likelihood_{Reference}}{Likelihood_{Response}}}\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}93}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{KL}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}93}]:} 0.09997757835164581
\end{Verbatim}
            
    {Notice } : new in {version 0.8.1}

    \hypertarget{mutual-information}{%
\subsubsection{Mutual Information}\label{mutual-information}}

    Mutual information is defined Kullback-Lieblier divergence, between the
product of the individual distributions and the joint distribution.
Mutual information is symmetric. We could also subtract the conditional
entropy of the reference given the response from the reference entropy
to get the same result. Section \ref{references}
Section \ref{references}

Wikipedia Page

    \[P^{'}(i,j)=\frac{Matrix(i,j)}{POP}\]

    \[Likelihood_{Reference}=\frac{P_i}{POP}\]

    \[Likelihood_{Response}=\frac{TOP_i}{POP}\]

    \[MI=-\sum_{i=1}^{|C|}\sum_{j=1}^{|C|}P^{'}(i,j)\times\log_{2}{\frac{P^{'}(i,j)}{Likelihood_{Reference}(i)\times Likelihood_{Response}(i) }}\]

    \[MI=Entropy_{Response}-Entropy_{Conditional}\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}94}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{MutualInformation}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}94}]:} 0.5242078379544428
\end{Verbatim}
            
    {Notice } : new in {version 0.8.1}

    \hypertarget{goodman-and-kruskals-lambda-a}{%
\subsubsection{Goodman and Kruskal's lambda
A}\label{goodman-and-kruskals-lambda-a}}

    In probability theory and statistics, Goodman \& Kruskal's lambda is a
measure of proportional reduction in error in cross tabulation analysis.
Section \ref{references}

Wikipedia page

    \[\lambda_A=\frac{(\sum_{j=1}^{|C|}Max(Matrix(-,j))-Max(P)}{POP-Max(P)}\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}95}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{LambdaA}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}95}]:} 0.42857142857142855
\end{Verbatim}
            
    {Notice } : new in {version 0.8.1}

    \hypertarget{goodman-and-kruskals-lambda-b}{%
\subsubsection{Goodman and Kruskal's lambda
B}\label{goodman-and-kruskals-lambda-b}}

    In probability theory and statistics, Goodman \& Kruskal's lambda is a
measure of proportional reduction in error in cross tabulation analysis.
Section \ref{references}

Wikipedia Page

    \[\lambda_B=\frac{(\sum_{i=1}^{|C|}Max(Matrix(i,-))-Max(TOP)}{POP-Max(TOP)}\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}96}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{LambdaB}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}96}]:} 0.16666666666666666
\end{Verbatim}
            
    {Notice } : new in {version 0.8.1}

    \hypertarget{soa1-strength-of-agreement-landis-and-koch-benchmark}{%
\subsubsection{SOA1 (Strength of Agreement, Landis and Koch
benchmark)}\label{soa1-strength-of-agreement-landis-and-koch-benchmark}}

    For more information visit Section \ref{references}

    Kappa

Strength of Agreement

0 \textgreater{}

Poor

0 - 0.20

Slight

0.21 -- 0.40

Fair

0.41 -- 0.60

Moderate

0.61 -- 0.80

Substantial

0.81 -- 1.00

Almost perfect

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}97}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{SOA1}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}97}]:} 'Fair'
\end{Verbatim}
            
    {Notice } : new in {version 0.3}

    \hypertarget{soa2-strength-of-agreement-fleiss-benchmark}{%
\subsubsection{SOA2 (Strength of Agreement, : Fleiss'
benchmark)}\label{soa2-strength-of-agreement-fleiss-benchmark}}

    For more information visit Section \ref{references}

    Kappa

Strength of Agreement

0.40 \textgreater{}

Poor

0.4 - 0.75

Intermediate to Good

More than 0.75

Excellent

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}98}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{SOA2}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}98}]:} 'Poor'
\end{Verbatim}
            
    {Notice } : new in {version 0.4}

    \hypertarget{soa3-strength-of-agreement-altmans-benchmark}{%
\subsubsection{SOA3 (Strength of Agreement, Altman's
benchmark)}\label{soa3-strength-of-agreement-altmans-benchmark}}

    For more information visit Section \ref{references}

    Kappa

Strength of Agreement

0.2 \textgreater{}

Poor

0.21 -- 0.40

Fair

0.41 -- 0.60

Moderate

0.61 -- 0.80

Good

0.81 -- 1.00

Very Good

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}99}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{SOA3}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}99}]:} 'Fair'
\end{Verbatim}
            
    {Notice } : new in {version 0.4}

    \hypertarget{soa4-strength-of-agreement-cicchettis-benchmark}{%
\subsubsection{SOA4 (Strength of Agreement, Cicchetti's
benchmark)}\label{soa4-strength-of-agreement-cicchettis-benchmark}}

    For more information visit Section \ref{references}

    Kappa

Strength of Agreement

0.4 \textgreater{}

Poor

0.4 -- 0.59

Fair

0.6 -- 0.74

Good

0.74 -- 1.00

Excellent

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}100}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{SOA4}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}100}]:} 'Poor'
\end{Verbatim}
            
    {Notice } : new in {version 0.7}

    \hypertarget{overall_acc}{%
\subsubsection{Overall\_ACC}\label{overall_acc}}

    For more information visit Section \ref{references}

    \[ACC_{Overall}=\frac{\sum_{i=1}^{|C|}TP_i}{POP}\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}101}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{Overall\PYZus{}ACC}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}101}]:} 0.5833333333333334
\end{Verbatim}
            
    {Notice } : new in {version 0.4}

    \hypertarget{overall_racc}{%
\subsubsection{Overall\_RACC}\label{overall_racc}}

    For more information visit Section \ref{references}

    \[RACC_{Overall}=\sum_{i=1}^{|C|}RACC_i\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}102}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{Overall\PYZus{}RACC}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}102}]:} 0.3541666666666667
\end{Verbatim}
            
    {Notice } : new in {version 0.4}

    \hypertarget{overall_raccu}{%
\subsubsection{Overall\_RACCU}\label{overall_raccu}}

    For more information visit Section \ref{references}

    \[RACCU_{Overall}=\sum_{i=1}^{|C|}RACCU_i\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}103}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{Overall\PYZus{}RACCU}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}103}]:} 0.3645833333333333
\end{Verbatim}
            
    {Notice } : new in {version 0.8.1}

    \hypertarget{ppv_micro}{%
\subsubsection{PPV\_Micro}\label{ppv_micro}}

    For more information visit Section \ref{references}

    \[PPV_{Micro}=\frac{\sum_{i=1}^{|C|}TP_i}{\sum_{i=1}^{|C|}TP_i+FP_i}\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}104}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{PPV\PYZus{}Micro}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}104}]:} 0.5833333333333334
\end{Verbatim}
            
    {Notice } : new in {version 0.4}

    \hypertarget{tpr_micro}{%
\subsubsection{TPR\_Micro}\label{tpr_micro}}

    For more information visit Section \ref{references}

    \[TPR_{Micro}=\frac{\sum_{i=1}^{|C|}TP_i}{\sum_{i=1}^{|C|}TP_i+FN_i}\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}105}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{TPR\PYZus{}Micro}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}105}]:} 0.5833333333333334
\end{Verbatim}
            
    {Notice } : new in {version 0.4}

    \hypertarget{ppv_macro}{%
\subsubsection{PPV\_Macro}\label{ppv_macro}}

    For more information visit Section \ref{references}

    \[PPV_{Macro}=\frac{1}{|C|}\sum_{i=1}^{|C|}\frac{TP_i}{TP_i+FP_i}\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}106}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{PPV\PYZus{}Macro}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}106}]:} 0.611111111111111
\end{Verbatim}
            
    {Notice } : new in {version 0.4}

    \hypertarget{tpr_macro}{%
\subsubsection{TPR\_Macro}\label{tpr_macro}}

    For more information visit Section \ref{references}

    \[TPR_{Macro}=\frac{1}{|C|}\sum_{i=1}^{|C|}\frac{TP_i}{TP_i+FN_i}\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}107}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{TPR\PYZus{}Macro}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}107}]:} 0.5666666666666668
\end{Verbatim}
            
    {Notice } : new in {version 0.4}

    \hypertarget{overall_j}{%
\subsubsection{Overall\_J}\label{overall_j}}

    For more information visit Section \ref{references}

    \[J_{Mean}=\frac{1}{|C|}\sum_{i=1}^{|C|}J_i\]

    \[J_{Sum}=\sum_{i=1}^{|C|}J_i\]

    \[J_{Overall}=(J_{Sum},J_{Mean})\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}108}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{Overall\PYZus{}J}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}108}]:} (1.225, 0.4083333333333334)
\end{Verbatim}
            
    {Notice } : new in {version 0.9}

    \hypertarget{hamming-loss}{%
\subsubsection{Hamming Loss}\label{hamming-loss}}

    The hamming\_loss computes the average Hamming loss or Hamming distance
between two sets of samples. Section \ref{references}

    \[L_{Hamming}=\frac{1}{POP}\sum_{i=1}^{|P|}1(y_i \neq \widehat{y}_i)\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}109}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{HammingLoss}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}109}]:} 0.41666666666666663
\end{Verbatim}
            
    {Notice } : new in {version 1.0}

    \hypertarget{zero-one-loss}{%
\subsubsection{Zero-one Loss}\label{zero-one-loss}}

    For more information visit Section \ref{references}

    \[L_{0-1}=\sum_{i=1}^{|P|}1(y_i \neq \widehat{y}_i)\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}110}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{ZeroOneLoss}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}110}]:} 5
\end{Verbatim}
            
    {Notice } : new in {version 1.1}

    \hypertarget{nir-no-information-rate}{%
\subsubsection{NIR (No Information
Rate)}\label{nir-no-information-rate}}

    The no information error rate is the error rate when the input and
output are independent

    \[NIR=\frac{1}{POP}Max(P)\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}111}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{NIR}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}111}]:} 0.4166666666666667
\end{Verbatim}
            
    {Notice } : new in {version 1.2}

    \hypertarget{p-value}{%
\subsubsection{P-Value}\label{p-value}}

    For more information visit Section \ref{references}

    \[x=\sum_{i=1}^{|C|}TP_{i}\]

    \[p=NIR\]

    \[n=POP\]

    \[P-Value_{(ACC > NIR)}=1-\sum_{i=1}^{x}\left(\begin{array}{c}n\\ i\end{array}\right)p^{i}(1-p)^{n-i}\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}112}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{PValue}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}112}]:} 0.18926430237560654
\end{Verbatim}
            
    {Notice } : new in {version 1.2}

    \hypertarget{overall_cen}{%
\subsubsection{Overall\_CEN}\label{overall_cen}}

    For more information visit Section \ref{references}

    \[P_j=\frac{\sum_{k=1}^{|C|}(Matrix(j,k)+Matrix(k,j))}{2\sum_{k,l=1}^{|C|}Matrix(k,l)}\]

    \[CEN_{Overall}=\sum_{j=1}^{|C|}(P_jCEN_j)\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}113}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{Overall\PYZus{}CEN}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}113}]:} 0.4638112995385119
\end{Verbatim}
            
    {Notice } : new in {version 1.3}

    \hypertarget{overall_mcen}{%
\subsubsection{Overall\_MCEN}\label{overall_mcen}}

    For more information visit Section \ref{references}

    \[\alpha=\begin{cases}1 & |C| > 2\\0 & |C| = 2\end{cases}\]

    \[P_j=\frac{\sum_{k=1}^{|C|}(Matrix(j,k)+Matrix(k,j))-Matrix(j,j)}{2\sum_{k,l=1}^{|C|}Matrix(k,l)-\alpha \sum_{k=1}^{|C|}Matrix(k,k)}\]

    \[MCEN_{Overall}=\sum_{j=1}^{|C|}(P_jMCEN_j)\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}114}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{Overall\PYZus{}MCEN}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}114}]:} 0.5189369467580801
\end{Verbatim}
            
    {Notice } : new in {version 1.3}

    \hypertarget{overall_mcc}{%
\subsubsection{Overall\_MCC}\label{overall_mcc}}

    For more information visit Section \ref{references}
Section \ref{references}

    \[MCC_{Overall}=\frac{cov(X,Y)}{\sqrt{cov(X,X).cov(Y,Y)}}\]

    \[cov(X,Y)=\sum_{i,j,k=1}^{|C|}Matrix(i,i)Matrix(k,j)-Matrix(j,i)Matrix(i,k)\]

    \[cov(X,X) = \sum_{i=1}^{|C|}[(\sum_{j=1}^{|C|}Matrix(j,i))(\sum_{k,l=1,k\neq i}^{|C|}Matrix(l,k))]\]

    \[cov(Y,Y) = \sum_{i=1}^{|C|}[(\sum_{j=1}^{|C|}Matrix(i,j))(\sum_{k,l=1,k\neq i}^{|C|}Matrix(k,l))]\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}115}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{Overall\PYZus{}MCC}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}115}]:} 0.36666666666666664
\end{Verbatim}
            
    {Notice } : new in {version 1.4}

    \hypertarget{rr-global-performance-index}{%
\subsubsection{RR (Global Performance
Index)}\label{rr-global-performance-index}}

    For more information visit Section \ref{references}

    \[RR=\frac{1}{|C|}\sum_{i,j=1}^{|C|}Matrix(i,j)\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}116}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{RR}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}116}]:} 4.0
\end{Verbatim}
            
    {Notice } : new in {version 1.4}

    \hypertarget{cba-class-balance-accuracy}{%
\subsubsection{CBA (Class Balance
Accuracy)}\label{cba-class-balance-accuracy}}

    For more information visit Section \ref{references}

    \[CBA=\frac{\sum_{i=1}^{|C|}\frac{Matrix(i,i)}{Max(TOP_i,P_i)}}{|C|}\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}117}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{CBA}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}117}]:} 0.4777777777777778
\end{Verbatim}
            
    {Notice } : new in {version 1.4}

    \hypertarget{aunu}{%
\subsubsection{AUNU}\label{aunu}}

    When dealing with multiclass problems, a global measure of
classification performances based on the ROC approach (AUNU) has been
proposed as the average of single-class measures.
Section \ref{references}

    \[AUNU=\frac{\sum_{i=1}^{|C|}AUC_i}{|C|}\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}118}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{AUNU}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}118}]:} 0.6785714285714285
\end{Verbatim}
            
    {Notice } : new in {version 1.4}

    \hypertarget{aunp}{%
\subsubsection{AUNP}\label{aunp}}

    Another option (AUNP) is that of averaging the AUCg values with weights
proportional to the number of samples experimentally belonging to each
class, that is, the a priori class distribution.
Section \ref{references}

    \[AUNP=\sum_{i=1}^{|C|}\frac{P_i}{POP}AUC_i\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}119}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{AUNP}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}119}]:} 0.6857142857142857
\end{Verbatim}
            
    {Notice } : new in {version 1.4}

    \hypertarget{rci-relative-classifier-information}{%
\subsubsection{RCI (Relative Classifier
Information)}\label{rci-relative-classifier-information}}

    Performance of different classifiers on the same domain can be measured
by comparing relative classifier information while classifier
information (mutual information) can be used for comparison across
different decision problems. Section \ref{references}
Section \ref{references}

    \[H_d=-\sum_{i=1}^{|C|}(\frac{\sum_{l=1}^{|C|}Matrix(i,l)}{\sum_{h,k=1}^{|C|}Matrix(h,k)}log_2\frac{\sum_{l=1}^{|C|}Matrix(i,l)}{\sum_{h,k=1}^{|C|}Matrix(h,k)})=Entropy_{Reference}\]

    \[H_o=\sum_{j=1}^{|C|}(\frac{\sum_{k=1}^{|C|}Matrix(k,j)}{\sum_{h,l=0}^{|C|}Matrix(h,l)}H_{oj})=Entropy_{Conditional}\]

    \[H_{oj}=-\sum_{i=1}^{|C|}(\frac{Matrix(i,j)}{\sum_{k=1}^{|C|}Matrix(k,j)}log_2\frac{Matrix(i,j)}{\sum_{k=1}^{|C|}Matrix(k,j)})\]

    \[RCI=\frac{H_d-H_o}{H_d}=\frac{MI}{Entropy_{Reference}}\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}120}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{RCI}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}120}]:} 0.3533932006492363
\end{Verbatim}
            
    {Notice } : new in {version 1.5}

    \hypertarget{print}{%
\subsection{Print}\label{print}}

    \hypertarget{full}{%
\subsubsection{Full}\label{full}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}121}]:} \PY{n+nb}{print}\PY{p}{(}\PY{n}{cm}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Predict          L1    L2    L3    
Actual
L1               3     0     2     

L2               0     1     1     

L3               0     2     3     





Overall Statistics : 

95\% CI                                                           (0.30439,0.86228)
AUNP                                                             0.68571
AUNU                                                             0.67857
Bennett\_S                                                        0.375
CBA                                                              0.47778
Chi-Squared                                                      6.6
Chi-Squared DF                                                   4
Conditional Entropy                                              0.97579
Cramer\_V                                                         0.5244
Cross Entropy                                                    1.58333
Gwet\_AC1                                                         0.38931
Hamming Loss                                                     0.41667
Joint Entropy                                                    2.45915
KL Divergence                                                    0.09998
Kappa                                                            0.35484
Kappa 95\% CI                                                     (-0.07708,0.78675)
Kappa No Prevalence                                              0.16667
Kappa Standard Error                                             0.22036
Kappa Unbiased                                                   0.34426
Lambda A                                                         0.42857
Lambda B                                                         0.16667
Mutual Information                                               0.52421
NIR                                                              0.41667
Overall\_ACC                                                      0.58333
Overall\_CEN                                                      0.46381
Overall\_J                                                        (1.225,0.40833)
Overall\_MCC                                                      0.36667
Overall\_MCEN                                                     0.51894
Overall\_RACC                                                     0.35417
Overall\_RACCU                                                    0.36458
P-Value                                                          0.18926
PPV\_Macro                                                        0.61111
PPV\_Micro                                                        0.58333
Phi-Squared                                                      0.55
RCI                                                              0.35339
RR                                                               4.0
Reference Entropy                                                1.48336
Response Entropy                                                 1.5
Scott\_PI                                                         0.34426
Standard Error                                                   0.14232
Strength\_Of\_Agreement(Altman)                                    Fair
Strength\_Of\_Agreement(Cicchetti)                                 Poor
Strength\_Of\_Agreement(Fleiss)                                    Poor
Strength\_Of\_Agreement(Landis and Koch)                           Fair
TPR\_Macro                                                        0.56667
TPR\_Micro                                                        0.58333
Zero-one Loss                                                    5

Class Statistics :

Classes                                                          L1                      L2                      L3                      
ACC(Accuracy)                                                    0.83333                 0.75                    0.58333                 
AUC(Area under the roc curve)                                    0.8                     0.65                    0.58571                 
BM(Informedness or bookmaker informedness)                       0.6                     0.3                     0.17143                 
CEN(Confusion entropy)                                           0.25                    0.49658                 0.60442                 
DOR(Diagnostic odds ratio)                                       None                    4.0                     2.0                     
DP(Discriminant power)                                           None                    0.33193                 0.16597                 
DPI(Discriminant power interpretation)                           None                    Poor                    Poor                    
ERR(Error rate)                                                  0.16667                 0.25                    0.41667                 
F0.5(F0.5 score)                                                 0.88235                 0.35714                 0.51724                 
F1(F1 score - harmonic mean of precision and sensitivity)        0.75                    0.4                     0.54545                 
F2(F2 score)                                                     0.65217                 0.45455                 0.57692                 
FDR(False discovery rate)                                        0.0                     0.66667                 0.5                     
FN(False negative/miss/type 2 error)                             2                       1                       2                       
FNR(Miss rate or false negative rate)                            0.4                     0.5                     0.4                     
FOR(False omission rate)                                         0.22222                 0.11111                 0.33333                 
FP(False positive/type 1 error/false alarm)                      0                       2                       3                       
FPR(Fall-out or false positive rate)                             0.0                     0.2                     0.42857                 
G(G-measure geometric mean of precision and sensitivity)         0.7746                  0.40825                 0.54772                 
IS(Information score)                                            1.26303                 1.0                     0.26303                 
J(Jaccard index)                                                 0.6                     0.25                    0.375                   
LR+(Positive likelihood ratio)                                   None                    2.5                     1.4                     
LR-(Negative likelihood ratio)                                   0.4                     0.625                   0.7                     
MCC(Matthews correlation coefficient)                            0.68313                 0.2582                  0.16903                 
MCEN(Modified confusion entropy)                                 0.26439                 0.5                     0.6875                  
MK(Markedness)                                                   0.77778                 0.22222                 0.16667                 
N(Condition negative)                                            7                       10                      7                       
NPV(Negative predictive value)                                   0.77778                 0.88889                 0.66667                 
P(Condition positive or support)                                 5                       2                       5                       
PLRI(Positive likelihood ratio interpretation)                   None                    Poor                    Poor                    
POP(Population)                                                  12                      12                      12                      
PPV(Precision or positive predictive value)                      1.0                     0.33333                 0.5                     
PRE(Prevalence)                                                  0.41667                 0.16667                 0.41667                 
RACC(Random accuracy)                                            0.10417                 0.04167                 0.20833                 
RACCU(Random accuracy unbiased)                                  0.11111                 0.0434                  0.21007                 
TN(True negative/correct rejection)                              7                       8                       4                       
TNR(Specificity or true negative rate)                           1.0                     0.8                     0.57143                 
TON(Test outcome negative)                                       9                       9                       6                       
TOP(Test outcome positive)                                       3                       3                       6                       
TP(True positive/hit)                                            3                       1                       3                       
TPR(Sensitivity, recall, hit rate, or true positive rate)        0.6                     0.5                     0.6                     
Y(Youden index)                                                  0.6                     0.3                     0.17143                 
dInd(Distance index)                                             0.4                     0.53852                 0.58624                 
sInd(Similarity index)                                           0.71716                 0.61921                 0.58547                 


    \end{Verbatim}

    \hypertarget{matrix}{%
\subsubsection{Matrix}\label{matrix}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}122}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{print\PYZus{}matrix}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Predict          L1    L2    L3    
Actual
L1               3     0     2     

L2               0     1     1     

L3               0     2     3     



    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}123}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{matrix}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}123}]:} \{0: \{0: 3, 1: 0, 2: 2\},
           1: \{0: 0, 1: 1, 2: 1\},
           2: \{0: 0, 1: 2, 2: 3\},
           'L1': \{'L1': 3, 'L2': 0, 'L3': 2\},
           'L2': \{'L1': 0, 'L2': 1, 'L3': 1\},
           'L3': \{'L1': 0, 'L2': 2, 'L3': 3\}\}
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}124}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{print\PYZus{}matrix}\PY{p}{(}\PY{n}{one\PYZus{}vs\PYZus{}all}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}\PY{n}{class\PYZus{}name} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Predict          L1    L2    L3    
Actual
L1               3     0     2     

L2               0     1     1     

L3               0     2     3     



    \end{Verbatim}

    {Notice } : \texttt{one\_vs\_all} option --\textgreater{} new in
{version 1.4 }

    {Notice } : \texttt{matrix()} renamed to \texttt{print\_matrix()} and
\texttt{matrix} return confusion matrix as dict from {version 1.5}

    \hypertarget{normalized-matrix}{%
\subsubsection{Normalized Matrix}\label{normalized-matrix}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}125}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{print\PYZus{}normalized\PYZus{}matrix}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Predict          L1     L2     L3     
Actual
L1               0.6    0.0    0.4    

L2               0.0    0.5    0.5    

L3               0.0    0.4    0.6    



    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}126}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{normalized\PYZus{}matrix}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}126}]:} \{0: \{0: 0.6, 1: 0.0, 2: 0.4\},
           1: \{0: 0.0, 1: 0.5, 2: 0.5\},
           2: \{0: 0.0, 1: 0.4, 2: 0.6\},
           'L1': \{'L1': 0.6, 'L2': 0.0, 'L3': 0.4\},
           'L2': \{'L1': 0.0, 'L2': 0.5, 'L3': 0.5\},
           'L3': \{'L1': 0.0, 'L2': 0.4, 'L3': 0.6\}\}
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}127}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{print\PYZus{}normalized\PYZus{}matrix}\PY{p}{(}\PY{n}{one\PYZus{}vs\PYZus{}all}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}\PY{n}{class\PYZus{}name} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Predict          L1     L2     L3     
Actual
L1               0.6    0.0    0.4    

L2               0.0    0.5    0.5    

L3               0.0    0.4    0.6    



    \end{Verbatim}

    {Notice } : \texttt{one\_vs\_all} option --\textgreater{} new in
{version 1.4 }

    {Notice } : \texttt{normalized\_matrix()} renamed to
\texttt{print\_normalized\_matrix()} and \texttt{normalized\_matrix}
return normalized confusion matrix as dict from {version 1.5}

    \hypertarget{stat}{%
\subsubsection{Stat}\label{stat}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}128}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{stat}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Overall Statistics : 

95\% CI                                                           (0.30439,0.86228)
AUNP                                                             0.68571
AUNU                                                             0.67857
Bennett\_S                                                        0.375
CBA                                                              0.47778
Chi-Squared                                                      6.6
Chi-Squared DF                                                   4
Conditional Entropy                                              0.97579
Cramer\_V                                                         0.5244
Cross Entropy                                                    1.58333
Gwet\_AC1                                                         0.38931
Hamming Loss                                                     0.41667
Joint Entropy                                                    2.45915
KL Divergence                                                    0.09998
Kappa                                                            0.35484
Kappa 95\% CI                                                     (-0.07708,0.78675)
Kappa No Prevalence                                              0.16667
Kappa Standard Error                                             0.22036
Kappa Unbiased                                                   0.34426
Lambda A                                                         0.42857
Lambda B                                                         0.16667
Mutual Information                                               0.52421
NIR                                                              0.41667
Overall\_ACC                                                      0.58333
Overall\_CEN                                                      0.46381
Overall\_J                                                        (1.225,0.40833)
Overall\_MCC                                                      0.36667
Overall\_MCEN                                                     0.51894
Overall\_RACC                                                     0.35417
Overall\_RACCU                                                    0.36458
P-Value                                                          0.18926
PPV\_Macro                                                        0.61111
PPV\_Micro                                                        0.58333
Phi-Squared                                                      0.55
RCI                                                              0.35339
RR                                                               4.0
Reference Entropy                                                1.48336
Response Entropy                                                 1.5
Scott\_PI                                                         0.34426
Standard Error                                                   0.14232
Strength\_Of\_Agreement(Altman)                                    Fair
Strength\_Of\_Agreement(Cicchetti)                                 Poor
Strength\_Of\_Agreement(Fleiss)                                    Poor
Strength\_Of\_Agreement(Landis and Koch)                           Fair
TPR\_Macro                                                        0.56667
TPR\_Micro                                                        0.58333
Zero-one Loss                                                    5

Class Statistics :

Classes                                                          L1                      L2                      L3                      
ACC(Accuracy)                                                    0.83333                 0.75                    0.58333                 
AUC(Area under the roc curve)                                    0.8                     0.65                    0.58571                 
BM(Informedness or bookmaker informedness)                       0.6                     0.3                     0.17143                 
CEN(Confusion entropy)                                           0.25                    0.49658                 0.60442                 
DOR(Diagnostic odds ratio)                                       None                    4.0                     2.0                     
DP(Discriminant power)                                           None                    0.33193                 0.16597                 
DPI(Discriminant power interpretation)                           None                    Poor                    Poor                    
ERR(Error rate)                                                  0.16667                 0.25                    0.41667                 
F0.5(F0.5 score)                                                 0.88235                 0.35714                 0.51724                 
F1(F1 score - harmonic mean of precision and sensitivity)        0.75                    0.4                     0.54545                 
F2(F2 score)                                                     0.65217                 0.45455                 0.57692                 
FDR(False discovery rate)                                        0.0                     0.66667                 0.5                     
FN(False negative/miss/type 2 error)                             2                       1                       2                       
FNR(Miss rate or false negative rate)                            0.4                     0.5                     0.4                     
FOR(False omission rate)                                         0.22222                 0.11111                 0.33333                 
FP(False positive/type 1 error/false alarm)                      0                       2                       3                       
FPR(Fall-out or false positive rate)                             0.0                     0.2                     0.42857                 
G(G-measure geometric mean of precision and sensitivity)         0.7746                  0.40825                 0.54772                 
IS(Information score)                                            1.26303                 1.0                     0.26303                 
J(Jaccard index)                                                 0.6                     0.25                    0.375                   
LR+(Positive likelihood ratio)                                   None                    2.5                     1.4                     
LR-(Negative likelihood ratio)                                   0.4                     0.625                   0.7                     
MCC(Matthews correlation coefficient)                            0.68313                 0.2582                  0.16903                 
MCEN(Modified confusion entropy)                                 0.26439                 0.5                     0.6875                  
MK(Markedness)                                                   0.77778                 0.22222                 0.16667                 
N(Condition negative)                                            7                       10                      7                       
NPV(Negative predictive value)                                   0.77778                 0.88889                 0.66667                 
P(Condition positive or support)                                 5                       2                       5                       
PLRI(Positive likelihood ratio interpretation)                   None                    Poor                    Poor                    
POP(Population)                                                  12                      12                      12                      
PPV(Precision or positive predictive value)                      1.0                     0.33333                 0.5                     
PRE(Prevalence)                                                  0.41667                 0.16667                 0.41667                 
RACC(Random accuracy)                                            0.10417                 0.04167                 0.20833                 
RACCU(Random accuracy unbiased)                                  0.11111                 0.0434                  0.21007                 
TN(True negative/correct rejection)                              7                       8                       4                       
TNR(Specificity or true negative rate)                           1.0                     0.8                     0.57143                 
TON(Test outcome negative)                                       9                       9                       6                       
TOP(Test outcome positive)                                       3                       3                       6                       
TP(True positive/hit)                                            3                       1                       3                       
TPR(Sensitivity, recall, hit rate, or true positive rate)        0.6                     0.5                     0.6                     
Y(Youden index)                                                  0.6                     0.3                     0.17143                 
dInd(Distance index)                                             0.4                     0.53852                 0.58624                 
sInd(Similarity index)                                           0.71716                 0.61921                 0.58547                 


    \end{Verbatim}

    {Notice } : \texttt{cm.params()} in prev versions (0.2\textgreater{})

    \hypertarget{save}{%
\subsection{Save}\label{save}}

    \hypertarget{pycm-file}{%
\subsubsection{.pycm file}\label{pycm-file}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}129}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{save\PYZus{}stat}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cm1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}129}]:} \{'Message': 'D:\textbackslash{}\textbackslash{}For Asus Laptop\textbackslash{}\textbackslash{}projects\textbackslash{}\textbackslash{}pycm\textbackslash{}\textbackslash{}Document\textbackslash{}\textbackslash{}cm1.pycm',
           'Status': True\}
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}130}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{save\PYZus{}stat}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cm1asdasd/}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}130}]:} \{'Message': "[Errno 2] No such file or directory: 'cm1asdasd/.pycm'",
           'Status': False\}
\end{Verbatim}
            
    {Notice } : new in {version 0.4}

    \hypertarget{html}{%
\subsubsection{HTML}\label{html}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}131}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{save\PYZus{}html}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cm1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}131}]:} \{'Message': 'D:\textbackslash{}\textbackslash{}For Asus Laptop\textbackslash{}\textbackslash{}projects\textbackslash{}\textbackslash{}pycm\textbackslash{}\textbackslash{}Document\textbackslash{}\textbackslash{}cm1.html',
           'Status': True\}
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}132}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{save\PYZus{}html}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cm1asdasd/}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}132}]:} \{'Message': "[Errno 2] No such file or directory: 'cm1asdasd/.html'",
           'Status': False\}
\end{Verbatim}
            
    {Notice } : new in {version 0.5}

    \hypertarget{csv}{%
\subsubsection{CSV}\label{csv}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}133}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{save\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cm1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}133}]:} \{'Message': 'D:\textbackslash{}\textbackslash{}For Asus Laptop\textbackslash{}\textbackslash{}projects\textbackslash{}\textbackslash{}pycm\textbackslash{}\textbackslash{}Document\textbackslash{}\textbackslash{}cm1.csv',
           'Status': True\}
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}134}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{save\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cm1asdasd/}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}134}]:} \{'Message': "[Errno 2] No such file or directory: 'cm1asdasd/.csv'",
           'Status': False\}
\end{Verbatim}
            
    {Notice } : new in {version 0.6}

    \hypertarget{obj}{%
\subsubsection{OBJ}\label{obj}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}135}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{save\PYZus{}obj}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cm1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}135}]:} \{'Message': 'D:\textbackslash{}\textbackslash{}For Asus Laptop\textbackslash{}\textbackslash{}projects\textbackslash{}\textbackslash{}pycm\textbackslash{}\textbackslash{}Document\textbackslash{}\textbackslash{}cm1.obj',
           'Status': True\}
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}136}]:} \PY{n}{cm}\PY{o}{.}\PY{n}{save\PYZus{}obj}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cm1asdasd/}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}136}]:} \{'Message': "[Errno 2] No such file or directory: 'cm1asdasd/.obj'",
           'Status': False\}
\end{Verbatim}
            
    {Notice } : new in {version 0.9.5}

    \hypertarget{input-errors}{%
\subsection{Input Errors}\label{input-errors}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}137}]:} \PY{k}{try}\PY{p}{:}
              \PY{n}{cm2}\PY{o}{=}\PY{n}{ConfusionMatrix}\PY{p}{(}\PY{n}{y\PYZus{}actu}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}
          \PY{k}{except} \PY{n}{pycmVectorError} \PY{k}{as} \PY{n}{e}\PY{p}{:}
              \PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{str}\PY{p}{(}\PY{n}{e}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Input Vectors Must Be List

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}138}]:} \PY{k}{try}\PY{p}{:}
              \PY{n}{cm3}\PY{o}{=}\PY{n}{ConfusionMatrix}\PY{p}{(}\PY{n}{y\PYZus{}actu}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
          \PY{k}{except} \PY{n}{pycmVectorError} \PY{k}{as} \PY{n}{e}\PY{p}{:}
              \PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{str}\PY{p}{(}\PY{n}{e}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Input Vectors Must Be The Same Length

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}139}]:} \PY{k}{try}\PY{p}{:}
              \PY{n}{cm\PYZus{}4} \PY{o}{=} \PY{n}{ConfusionMatrix}\PY{p}{(}\PY{p}{[}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{p}{]}\PY{p}{)}
          \PY{k}{except} \PY{n}{pycmVectorError} \PY{k}{as} \PY{n}{e}\PY{p}{:}
              \PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{str}\PY{p}{(}\PY{n}{e}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Input Vectors Are Empty

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}140}]:} \PY{k}{try}\PY{p}{:}
              \PY{n}{cm\PYZus{}5} \PY{o}{=} \PY{n}{ConfusionMatrix}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
          \PY{k}{except} \PY{n}{pycmVectorError} \PY{k}{as} \PY{n}{e}\PY{p}{:}
              \PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{str}\PY{p}{(}\PY{n}{e}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Input Vectors Must Be The Same Length

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}141}]:} \PY{k}{try}\PY{p}{:}
              \PY{n}{cm3}\PY{o}{=}\PY{n}{ConfusionMatrix}\PY{p}{(}\PY{n}{matrix}\PY{o}{=}\PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}\PY{p}{)}
          \PY{k}{except} \PY{n}{pycmMatrixError} \PY{k}{as} \PY{n}{e}\PY{p}{:}
              \PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{str}\PY{p}{(}\PY{n}{e}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Input Confusion Matrix Format Error

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}142}]:} \PY{k}{try}\PY{p}{:}
              \PY{n}{cm\PYZus{}4}\PY{o}{=}\PY{n}{ConfusionMatrix}\PY{p}{(}\PY{n}{matrix}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{\PYZob{}}\PY{l+m+mi}{1}\PY{p}{:}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}\PY{l+m+mi}{2}\PY{p}{\PYZcb{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}\PY{p}{\PYZob{}}\PY{l+m+mi}{1}\PY{p}{:}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}\PY{l+m+mi}{3}\PY{p}{\PYZcb{}}\PY{p}{\PYZcb{}}\PY{p}{)}
          \PY{k}{except} \PY{n}{pycmMatrixError} \PY{k}{as} \PY{n}{e}\PY{p}{:}
              \PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{str}\PY{p}{(}\PY{n}{e}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Input Matrix Classes Must Be Same Type

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}143}]:} \PY{k}{try}\PY{p}{:}
              \PY{n}{cm\PYZus{}5}\PY{o}{=}\PY{n}{ConfusionMatrix}\PY{p}{(}\PY{n}{matrix}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{\PYZob{}}\PY{l+m+mi}{1}\PY{p}{:}\PY{l+m+mi}{2}\PY{p}{\PYZcb{}}\PY{p}{\PYZcb{}}\PY{p}{)}
          \PY{k}{except} \PY{n}{pycmVectorError} \PY{k}{as} \PY{n}{e}\PY{p}{:}
              \PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{str}\PY{p}{(}\PY{n}{e}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Number Of Classes < 2

    \end{Verbatim}

    {Notice } : updated in {version 0.8}

    \hypertarget{examples}{%
\subsection{Examples}\label{examples}}

    \hypertarget{example-1-comparison-of-three-different-classifiers}{%
\subsubsection{Example-1 (Comparison of three different
classifiers)}\label{example-1-comparison-of-three-different-classifiers}}

\begin{itemize}
\tightlist
\item
  \href{https://nbviewer.jupyter.org/github/sepandhaghighi/pycm/blob/master/Document/Example1.ipynb}{Jupyter
  Notebook}
\item
  \href{http://www.shaghighi.ir/pycm/doc/Example1.html}{HTML}
\end{itemize}

\hypertarget{example-2-how-to-plot-via-matplotlib}{%
\subsubsection{Example-2 (How to plot via
matplotlib)}\label{example-2-how-to-plot-via-matplotlib}}

\begin{itemize}
\tightlist
\item
  \href{https://nbviewer.jupyter.org/github/sepandhaghighi/pycm/blob/master/Document/Example2.ipynb}{Jupyter
  Notebook}
\item
  \href{http://www.shaghighi.ir/pycm/doc/Example2.html}{HTML}
\end{itemize}

\hypertarget{example-3-activation-threshold}{%
\subsubsection{Example-3 (Activation
Threshold)}\label{example-3-activation-threshold}}

\begin{itemize}
\tightlist
\item
  \href{https://nbviewer.jupyter.org/github/sepandhaghighi/pycm/blob/master/Document/Example3.ipynb}{Jupyter
  Notebook}
\item
  \href{http://www.shaghighi.ir/pycm/doc/Example3.html}{HTML}
\end{itemize}

\hypertarget{example-4-activation-threshold}{%
\subsubsection{Example-4 (Activation
Threshold)}\label{example-4-activation-threshold}}

\begin{itemize}
\tightlist
\item
  \href{https://nbviewer.jupyter.org/github/sepandhaghighi/pycm/blob/master/Document/Example4.ipynb}{Jupyter
  Notebook}
\item
  \href{http://www.shaghighi.ir/pycm/doc/Example4.html}{HTML}
\end{itemize}

\hypertarget{example-5-sample-weights}{%
\subsubsection{Example-5 (Sample
Weights)}\label{example-5-sample-weights}}

\begin{itemize}
\tightlist
\item
  \href{https://nbviewer.jupyter.org/github/sepandhaghighi/pycm/blob/master/Document/Example5.ipynb}{Jupyter
  Notebook}
\item
  \href{http://www.shaghighi.ir/pycm/doc/Example5.html}{HTML}
\end{itemize}

    \hypertarget{references}{%
\subsection{References}\label{references}}

    1- J. R. Landis, G. G. Koch, ``The measurement of observer agreement for
categorical data. Biometrics,'' in International Biometric Society,
pp.~159--174, 1977.

2- D. M. W. Powers, ``Evaluation: from precision, recall and f-measure
to roc, informedness, markedness \& correlation,'' in Journal of Machine
Learning Technologies, pp.37-63, 2011.

3- C. Sammut, G. Webb, ``Encyclopedia of Machine Learning'' in Springer,
2011.

4- J. L. Fleiss, ``Measuring nominal scale agreement among many
raters,'' in Psychological Bulletin, pp.~378-382.

5- D.G. Altman, ``Practical Statistics for Medical Research,'' in
Chapman and Hall, 1990.

6- K. L. Gwet, ``Computing inter-rater reliability and its variance in
the presence of high agreement,'' in The British Journal of Mathematical
and Statistical Psychology, pp.~29--48, 2008.''

7- W. A. Scott, ``Reliability of content analysis: The case of nominal
scaling,'' in Public Opinion Quarterly, pp.~321--325, 1955.

8- E. M. Bennett, R. Alpert, and A. C. Goldstein, ``Communication
through limited response questioning,'' in The Public Opinion Quarterly,
pp.~303--308, 1954.

9- D. V. Cicchetti, ``Guidelines, criteria, and rules of thumb for
evaluating normed and standardized assessment instruments in
psychology,'' in Psychological Assessment, pp.~284--290, 1994.

10- R.B. Davies, ``Algorithm AS155: The Distributions of a Linear
Combination of χ2 Random Variables,'' in Journal of the Royal
Statistical Society, pp.~323--333, 1980.

11- S. Kullback, R. A. Leibler ``On information and sufficiency,'' in
Annals of Mathematical Statistics, pp.~79--86, 1951.

12- L. A. Goodman, W. H. Kruskal, ``Measures of Association for Cross
Classifications, IV: Simplification of Asymptotic Variances,'' in
Journal of the American Statistical Association, pp.~415--421, 1972.

13- L. A. Goodman, W. H. Kruskal, ``Measures of Association for Cross
Classifications III: Approximate Sampling Theory,'' in Journal of the
American Statistical Association, pp.~310--364, 1963.

14- T. Byrt, J. Bishop and J. B. Carlin, ``Bias, prevalence, and
kappa,'' in Journal of Clinical Epidemiology pp.~423-429, 1993.

15- M. Shepperd, D. Bowes, and T. Hall, ``Researcher Bias: The Use of
Machine Learning in Software Defect Prediction,'' in IEEE Transactions
on Software Engineering, pp.~603-616, 2014.

16- X. Deng, Q. Liu, Y. Deng, and S. Mahadevan, ``An improved method to
construct basic probability assignment based on the confusion matrix for
classification problem,'' in Information Sciences, pp.250-261, 2016.

17- Wei, J.-M., Yuan, X.-Y., Hu, Q.-H., Wang, S.-Q.: A novel measure for
evaluating classifiers. Expert Systems with Applications, Vol 37,
3799--3809 (2010).

18- Kononenko I. and Bratko I. Information-based evaluation criterion
for classifier's performance. Machine Learning, 6:67--80, 1991.

19- Delgado R., Núñez-González J.D. (2019) Enhancing Confusion Entropy
as Measure for Evaluating Classifiers. In: Graña M. et al. (eds)
International Joint Conference SOCO'18-CISIS'18-ICEUTE'18.
SOCO'18-CISIS'18-ICEUTE'18 2018. Advances in Intelligent Systems and
Computing, vol 771. Springer, Cham

20- Gorodkin J (2004) Comparing two K-category assignments by a
K-category correlation coefficient. Computational Biology and Chemistry
28: 367--374

21- Freitas C.O.A., de Carvalho J.M., Oliveira J., Aires S.B.K.,
Sabourin R. (2007) Confusion Matrix Disagreement for Multiple
Classifiers. In: Rueda L., Mery D., Kittler J. (eds) Progress in Pattern
Recognition, Image Analysis and Applications. CIARP 2007. Lecture Notes
in Computer Science, vol 4756. Springer, Berlin, Heidelberg

22- Branco P., Torgo L., Ribeiro R.P. (2017) Relevance-Based Evaluation
Metrics for Multi-class Imbalanced Domains. In: Kim J., Shim K., Cao L.,
Lee JG., Lin X., Moon YS. (eds) Advances in Knowledge Discovery and Data
Mining. PAKDD 2017. Lecture Notes in Computer Science, vol 10234.
Springer, Cham

23- Ballabio, D., Grisoni, F. and Todeschini, R. (2018). Multivariate
comparison of classification performance measures. Chemometrics and
Intelligent Laboratory Systems, 174, pp.33-44.

24- Cohen, Jacob. 1960. A coefficient of agreement for nominal scales.
Educational And Psychological Measurement 20:37-46

25- Siegel, Sidney and N. John Castellan, Jr.~1988. Nonparametric
Statistics for the Behavioral Sciences. McGraw Hill.

26- Cramér, Harald. 1946. Mathematical Methods of Statistics. Princeton:
Princeton University Press, page 282 (Chapter 21. The two-dimensional
case)

27- Matthews, B. W. (1975). ``Comparison of the predicted and observed
secondary structure of T4 phage lysozyme''. Biochimica et Biophysica
Acta (BBA) - Protein Structure. 405 (2): 442--451.

28- Swets JA. (1973). ``The relative operating characteristic in
Psychology''. Science. 182 (14116): 990--1000.

29- Jaccard, Paul (1901), ``Étude comparative de la distribution florale
dans une portion des Alpes et des Jura'', Bulletin de la Société
Vaudoise des Sciences Naturelles, 37: 547--579.

30- Thomas M. Cover and Joy A. Thomas. 2006. Elements of Information
Theory (Wiley Series in Telecommunications and Signal Processing).
Wiley-Interscience, New York, NY, USA.

31- Keeping, E.S. (1962) Introduction to Statistical Inference. D. Van
Nostrand, Princeton, NJ.

32- Sindhwani V, Bhattacharge P, Rakshit S (2001) Information theoretic
feature crediting in multiclass Support Vector Machines. In: Grossman R,
Kumar V, editors, Proceedings First SIAM International Conference on
Data Mining, ICDM01. SIAM, pp.~1--18.

33- Bekkar, Mohamed \& Djema, Hassiba \& Alitouche, T.A.. (2013).
Evaluation measures for models assessment over imbalanced data sets.
Journal of Information Engineering and Applications. 3. 27-38.

34- Youden W, (1950),« Index for rating diagnostic tests »; Cancer, 3
:32--35


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
